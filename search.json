[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pavel Grigoryev projects",
    "section": "",
    "text": "Исследование надёжности заёмщиков\n\n\nИсследование влияния семейного положения и количества детей клиента на факт погашения кредита в срок для кредитного отдела банка.\n\n\n\n\n\n\n\n\n\n\nИсследование объявлений о продаже квартир\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/creditworthiness-research/creditworthiness_research.html",
    "href": "projects/creditworthiness-research/creditworthiness_research.html",
    "title": "Исследование надёжности заёмщиков",
    "section": "",
    "text": "Автор:\nГригорьев Павел\nОписание проекта:\nЗаказчик - кредитный отдел банка.\nНужно разобраться, влияет ли семейное положение и количество детей клиента на факт погашения кредита в срок.\nВходные данные от банка - статистика о платёжеспособности клиентов.\nРезультаты исследования будут учтены при построении модели кредитного скоринга - специальной системы, которая оценивает способность потенциального заёмщика вернуть кредит банку.\nЦель:\nСоставить рекомендации для кредитного отдела банка, которые будут учтены при построении модели кредитного скоринга.\nОпределить влияет ли семейное положение и количество детей клиента на факт погашения кредита в срок.\n{description}\nИсточники данных:\nСтатистика о платёжеспособности клиентов.\nГлавные выводы:\nАномалии и особенности в данных:\nОглавление\n1. Описание и изучение данных\n   1.1 Описание данных\n   1.2 Изучение данных\n     1.2.1 Изучение переменных\n     1.2.2 Изучение дубликатов\n     1.2.3 Изучение пропусков\n     1.2.4 Изучение выбросов\n   1.3 Промежуточный вывод\n 2. Предобработка данных\n   2.1 Выбор нужных столбцов для дальнейшей работы и нормализация таблицы\n   2.2 Обработка выбросов\n   2.3 Обработка пропусков\n   2.4 Обработка дубликатов\n   2.5 Приведение данных к удобной форме\n   2.6 Категоризация данных\n   2.7 Промежуточный вывод\n 3. Анализ корреляций между переменными\n   3.1 Исследование корреляционных связей\n   3.2 Промежуточный вывод\n 4. Анализ взаимосвязей переменных на графиках\n   4.1 Изучение зависимостей между числовыми переменными\n   4.2 Изучение зависимостей между категориальными переменными\n   4.3 Изучение зависимостей между числовыми и категориальными переменными\n   4.4 Промежуточный вывод\n 5. Формулирование и провера гипотез\n   5.1 Формулирование гипотез\n   5.2 Проверка гипотез\n   5.3 Промежуточный вывод\n 6. Общий вывод\nРекомендации: - Добавить контроль данных, чтобы не дублировались значения с разными регистрами в колонке с образованием. - Добавить уникальный идентификатор клиента, чтобы избежать дублирования строк. - Добавить проверку на отрицательные значения и на слишком большие значения в количестве детей при загрузке данных. - Выяснить откуда возникают отрицательные значения в трудовом стаже и добавить контроль ввода невалидных данных. - Выяснить причину нулевых значений в колонке возраста и добавить проверку на нулевые значения при загрузке данных. - Выяснить причину большого количества знаков после запятой в колонке дохода."
  },
  {
    "objectID": "projects/creditworthiness-research/creditworthiness_research.html#загрузка-библиотек",
    "href": "projects/creditworthiness-research/creditworthiness_research.html#загрузка-библиотек",
    "title": "Исследование надёжности заёмщиков",
    "section": "Загрузка библиотек ",
    "text": "Загрузка библиотек \n\n\nShow the code\n# %%capture\n# !pip install dash pingouin pyaspeller kaleido\n\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport pagri_data_tools  # type: ignore\n\n# import httpimport\n# with httpimport.github_repo('PAGriAnalytics', 'pagri_analytics_modules'):\n# import pagri_data_tools\n\n\n\nTraceback (most recent call last):\n\n  File ~/.conda/envs/pagri_conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577 in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  Cell In[2], line 4\n    import pagri_data_tools  # type: ignore\n\n  File ~/git_repos/pagri_analytics_modules/pagri_data_tools.py:7\n    from pagri_graphs import *\n\n  File ~/git_repos/pagri_analytics_modules/pagri_graphs.py:2030\n    , hoverlabel=dict(bgcolor=\"white\"), xaxis=dict(\n    ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "projects/creditworthiness-research/creditworthiness_research.html#описание-и-изучение-данных",
    "href": "projects/creditworthiness-research/creditworthiness_research.html#описание-и-изучение-данных",
    "title": "Исследование надёжности заёмщиков",
    "section": "1. Описание и изучение данных",
    "text": "1. Описание и изучение данных\n\n1.1 Описание данных\nвернуться к оглавлению\n\nchildren - количество детей в семье\ndays_employed - общий трудовой стаж в днях\ndob_years - возраст клиента в годах\neducation - уровень образования клиента\neducation_id - идентификатор уровня образования\nfamily_status - семейное положение\nfamily_status_id - идентификатор семейного положения\ngender - пол клиента\nincome_type - тип занятости\ndebt - имел ли задолженность по возврату кредитов\ntotal_income - ежемесячный доход\npurpose - цель получения кредита\n\n\n\n1.2 Изучение данных\n\n1.2.1 Изучение переменных\nвернуться к оглавлению\n\n\nShow the code\ndtype = {\n    \"education\": \"category\",\n    \"education_id\": \"category\",\n    \"family_status\": \"category\",\n    \"family_status_id\": \"category\",\n    \"gender\": \"category\",\n    \"income_type\": \"category\",\n    \"debt\": \"category\",\n}\ndf = pd.read_csv(\"https://code.s3.yandex.net/datasets/data.csv\", dtype=dtype)\ndf.sample(5, random_state=7)\n\n\n\n\n\n\n\n\n\nchildren\ndays_employed\ndob_years\neducation\neducation_id\nfamily_status\nfamily_status_id\ngender\nincome_type\ndebt\ntotal_income\npurpose\n\n\n\n\n4042\n1\n-2885.142188\n50\nсреднее\n1\nженат / замужем\n0\nF\nсотрудник\n0\n80236.028323\nприобретение автомобиля\n\n\n19177\n2\n-1803.080913\n36\nСреднее\n1\nженат / замужем\n0\nF\nсотрудник\n0\n163292.220004\nстроительство собственной недвижимости\n\n\n7372\n1\n-305.540665\n27\nСРЕДНЕЕ\n1\nгражданский брак\n1\nF\nсотрудник\n0\n69799.488812\nремонт жилью\n\n\n16245\n1\n-1593.946336\n50\nсреднее\n1\nженат / замужем\n0\nF\nсотрудник\n1\n107486.332934\nна покупку подержанного автомобиля\n\n\n11563\n0\n-1025.402943\n64\nвысшее\n0\nженат / замужем\n0\nM\nгосслужащий\n0\n706401.475790\nпрофильное образование\n\n\n\n\n\n\n\nИзменим название колонки dob_years на более понятное age\n\n\nShow the code\ndf = df.rename(columns={\"dob_years\": \"age\"})\n\n\nИзменим значения в столбце debt на ‘есть’, ‘нет’\n\n\nShow the code\ndf.debt = df.debt.apply(lambda x: \"есть\" if x == \"1\" else \"нет\").astype(\"category\")\n\n\nИзучим отдельно каждую колонку\n\n\nShow the code\ngen = pagri_data_tools.my_info_gen(df)\n\n\n вернуться к оглавлению\n\n\nShow the code\nnext(gen)\n\n\n\n\n\n\n\nTable 1: DataFrame\n\n\n\n\n\nRows\nFeatures\nRAM (Mb)\nDuplicates\nDupl (sub - origin)\n\n\n\n\n21 525\n12\n4\n54 (&lt;1%)\n0 (0%)\n\n\n\n\n\n\n\n\nНаблюдения:\n- В датафрейме есть строки дубликаты. 54 строки. Меньше 1 % от всего датафрейма.\nЕсли заменить все пробелы на 1, привести к нижнему регистру, то дополнительно появляется 31 дубликат.\nНужно детальнее изучить дубликаты.\n вернуться к оглавлению"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html",
    "title": "Исследование объявлений о продаже квартир",
    "section": "",
    "text": "Описание проекта:\nПроект направлен на исследование факторов, влияющих на ценообразование на рынке недвижимости в Санкт-Петербурге.\nВ ходе проекта будут выявлены ключевые параметры, которые определяют стоимость квартир.\nПолученные знания будут использованы для создания системы, которая будет отслеживать аномалии и предупреждать о возможных случаях мошенничества.\nАвтор:\nГригорьев Павел\nЦель:\nВыявить ключевые параметры, определяющие стоимость квартир, и разработать рекомендации для создания системы мониторинга,\nкоторая поможет выявлять аномалии в ценах и предупреждать о мошенничестве на рынке недвижимости.\nИсточники данных:\nДанные сервиса Яндекс.Недвижимость — архив объявлений о продаже квартир в Санкт-Петербурге и соседних населённых пунктах за несколько лет.\nУсловия проведения анализа днных:\nГлавные выводы:\nтут помещаем самое главное из общего вывода, примерно до полустраницы, чтобы не было сильно много и при этом указать все главные выводы Будет идеально, елси выводы на похожие темы будут рядом, то есть елси мы имеем несколько выводов о доходе, то лушче поместить их рядом - Женщины чаще возвращают кредит, чем мужчины. - Долги присутствуют у людей с разным доходом.\nАномалии и особенности в данных: - В датафрейме есть строки дубликаты. 54 строки. Меньше 1 % от всего датафрейма.\n- В столбце с количеством детей есть отрицательные значения. 47 штук. Меньше 1 процента от всего датафрейма. Также есть клиенты с 20 детьми.\nРекомендации:\n- Добавить контроль данных, чтобы не дублировались значения с разными регистрами в колонке с образованием. - Добавить уникальный идентификатор клиента, чтобы избежать дублирования строк.\nОглавление"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#загрузка-библиотек",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#загрузка-библиотек",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Загрузка библиотек",
    "text": "Загрузка библиотек\n\n\nShow the code\n%load_ext autoreload\n%autoreload 2\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport pagri_data_tools  # type: ignore"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#описание-и-изучение-данных",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#описание-и-изучение-данных",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Описание и изучение данных",
    "text": "Описание и изучение данных\n\nОписание данных\nПо каждой квартире на продажу доступны два вида данных. Первые вписаны пользователем, вторые — получены автоматически на основе картографических данных.\nНапример, расстояние до центра, аэропорта, ближайшего парка и водоёма.\n\nairports_nearest - расстояние до ближайшего аэропорта в метрах (м)\nbalcony - число балконов\nceiling_height - высота потолков (м)\ncityCenters_nearest - расстояние до центра города (м)\ndays_exposition - сколько дней было размещено объявление (от публикации до снятия)\nfirst_day_exposition - дата публикации\nfloor - этаж\nfloors_total - всего этажей в доме\nis_apartment - апартаменты (булев тип)\nkitchen_area - площадь кухни в квадратных метрах (м²)\nlast_price - цена на момент снятия с публикации\nliving_area - жилая площадь в квадратных метрах(м²)\nlocality_name - название населённого пункта\nopen_plan - свободная планировка (булев тип)\nparks_around3000 - число парков в радиусе 3 км\nparks_nearest - расстояние до ближайшего парка (м)\nponds_around3000 - число водоёмов в радиусе 3 км\nponds_nearest - расстояние до ближайшего водоёма (м)\nrooms - число комнат\nstudio - квартира-студия (булев тип)\ntotal_area - площадь квартиры в квадратных метрах (м²)\ntotal_images - число фотографий квартиры в объявлении\n\n\n\nИзучение данных\n\nИзучение переменных\nОставим тип float, так как у нас пропуски в данных и мы не можем преобразовать нужные столбцы в int\n\n\nShow the code\ndtype = {'is_apartment': 'category', 'studio': 'category', 'open_plan': 'category'}\ndf = pd.read_csv('https://code.s3.yandex.net/datasets/real_estate_data.csv', dtype=dtype, sep='\\t'\n                , parse_dates=['first_day_exposition'], date_format='%Y-%m-%dT%H:%M:%S')\ndf.rename(columns={'cityCenters_nearest': 'city_centers_nearest'}, inplace=True)\ndf.sample(5, random_state=7)\n\n\n\n  \n    \n\n\n\n\n\n\ntotal_images\nlast_price\ntotal_area\nfirst_day_exposition\nrooms\nceiling_height\nfloors_total\nliving_area\nfloor\nis_apartment\n...\nkitchen_area\nbalcony\nlocality_name\nairports_nearest\ncity_centers_nearest\nparks_around3000\nparks_nearest\nponds_around3000\nponds_nearest\ndays_exposition\n\n\n\n\n6029\n7\n4200000.0\n42.00\n2017-12-05\n2\n2.58\n5.0\n28.5\n5\nNaN\n...\n5.00\nNaN\nСанкт-Петербург\n36416.0\n9534.0\n2.0\n446.0\n3.0\n459.0\n18.0\n\n\n21836\n7\n3950000.0\n45.00\n2015-06-10\n2\nNaN\n9.0\n28.0\n7\nNaN\n...\n7.00\n0.0\nСанкт-Петербург\n27880.0\n12138.0\n1.0\n638.0\n1.0\n593.0\n437.0\n\n\n5698\n7\n450000.0\n42.00\n2017-07-31\n2\nNaN\n1.0\n23.0\n1\nNaN\n...\n5.80\nNaN\nгородской посёлок Будогощь\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n233.0\n\n\n9402\n11\n4900000.0\n55.00\n2017-06-19\n2\nNaN\n14.0\n32.0\n14\nNaN\n...\n8.50\n2.0\nСанкт-Петербург\n39931.0\n12834.0\n0.0\nNaN\n0.0\nNaN\n15.0\n\n\n2519\n6\n3511000.0\n39.41\n2018-08-07\n1\n2.65\n27.0\nNaN\n12\nNaN\n...\n11.28\n1.0\nпосёлок Мурино\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n60.0\n\n\n\n\n5 rows × 22 columns\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n  \n\n\nИзучим каждую колонку отдельно\n\n\nShow the code\ngen = pagri_data_tools.my_info_gen(df)\nnext(gen)\n\n\n\n\n\n\n\nTable 1: DataFrame\n\n\n\n\n\nRows\nFeatures\nRAM (Mb)\nDuplicates\nDupl (sub - origin)\n\n\n\n\n23 699\n22\n6\n---\n---\n\n\n\n\n\n\n\n\nНаблюдения: - Полных дубликатов в датафрейме нет\n\n\nShow the code\nnext(gen) \n\n\n\nВАЖНО\nЕсли увидели, что у нас в категориальной переменной одни и те же значения, но записанные с большой и с маленькой буквы, например,\nили другие проблемы с написанием одно и того же слова, что приводит к увеличению значений в категории.\nТо нужно сразу это убирать, так как дальнейший анализ будет страдать.\nДля дальнейшего анализа срауз приведем колонку education к нижнему регистру и удалим лишние пробелы\n\n\nДля дальнейшего анализа срауз приведем колонку education к нижнему регистру и удалим лишние пробелы\n\nНужно проверить, что после нормализации категориальный тип остался прежний\n\n\nShow the code\ndf.education = pagri_prep.normalize_string_series(df.education)\ndf.education.value_counts()\n\n\nсреднее                15233\nвысшее                  5260\nнеоконченное высшее      744\nначальное                282\nученая степень             6\nName: education, dtype: int64\n\n\n\n\nShow the code\ndf.education.dtype\n\n\n\nсделать предположения, почему могло так произойти, выдвигаем гипотезы\n\n\nпридумать способы проверки выдвинутых гипотез и записать\n\n\nесли у нас по оси x время, то проанализировать сезонность\n\n\nподумать а так и должно было получиться, основываясь на понимании физики параметра\n\n\nзафиксировать возможные рекомендации\n\n\nДля гистограмм, нужно понять почему именно такое распределение метрики.\nСовпадет это с логикой этой метрики.\n\n\nТакже когда строим гистограммы и вайолин плот, то не просто фиксируем, что есть тяжелые хвосты, разброс между квартилями такой-то.\nА думаем почему так, пытаемся связать это с физикой параметра. Должно быть физическое объяснение всех аномалий.\nЕсли объяснения нет, то возможно это инсайт.\n\n\nВажно убедиться, что у нас есть данные на все источники, которые заявлены. Например, мы изучаем источники трафика и у нас они в разных таблицах.\nНужно убедиться, что во всех таблицах есть все источники, и проверить нет ли аномалий, возможно какой-то сильно выбивается или какого-то вообще где-то нет.\n\n\nИ очень важно сверить, что периоды в разных таблицах (если у нас больше одной таблицы) совпадают.\n\n\nВажно проверить соответствуют ли временной период данных тому, который заявлен в задании,\nопределиться что будем делать с неполными периодами.\n\n\nВообще, когда у нас несколько таблиц и там есть категориальные переменные или время, то\nмы должны взять уникальные значения категориальных переменных из каждой таблицы (одниаковые переменные) и сравнить.\nКоличество уникальных должно совпадать, иначе нужно разбираться\nИ с верменем как минимум мин и макс даты должны совпадать до дня, а лушше до минуты часа\n\n\nОчень важно, если у нас есть стартовая дата чего-то и конечная, то обязательно нужно проверить,\nнет ли у нас записей, где конечная дата меньше стартовой.\n\n\nВажная проверка, если у нас есть категории и даты, то сгруппировать по категориями и вывести количество занчений, минимальную и максимальную дату\nТаким образом мы сразу поймем распределение в категории и\nувидем какие временные интервалы у каждой категории\nЕсли у нас все категории должны быть в один день, то мы поймем нет ли багов\n\n\nВообще очень важно смотреть не только на аномалии в значениях, но и аномалии в категориальных переменных.\nА тут аномалией будет отстутствие какого-то значения, хотя в описании или поставновке задачи оно есть.\nТакже совпадение количества значений категориальных переменных в разных таблицах.\n\n\nВнимательно посмотреть на столбцы, если есть столбцы, в которых могут быть потенциальные анамали, то проверить их.\nНапример, есть столбец возрасти стаж работы, проверить, что возраст больше стажа.\nИ подобные случаи.\n\n\nПроверка на нарушения уникальности\nУбедить, что столбцы, значения в которых не должны повторяться и должны быть уникальными, такие в действительности.\nСмотрим на результат функции my_info\n\n\nПроверка на ошибки целостности\nЕсли у нас есть столбцы, в которых значения должны совпдаать попарно, то проверяем на это\nget_non_matching_rows\n\n\n\nShow the code\npagri_data_tools.get_non_matching_rows()\n\n\n\nПроверка условий\nПроверьте, что данные в датафрейме удовлетворяют определенным условиям, таким как “возраст &gt; 18” или “страна == ‘Россия’”\n\n\n\nИзучение дубликатов\n\nНе забываем про ИИ.\nПишем список столбцов (именно что они значат, то есть образование, пол и прочее), говорим, что есть дубли.\nИ просим предложить причины этих дублей. Если видим важное, то используем для рекомендаций, выводов и замены дублей в предобработке.\n\n\nПроверяем на дубли\n- Важно помнить, что если у нас есть id и название товара, то названия товара все равно нужно проверить на дубли,\nвозможно у нас 2 ай ди с одним названием. - Также важно в каждой отдельной колонке проверить дубли и если их много, то посмотреть на соседние колонки, что там происходит - Дубликаты часто носят скрытый характер.\nТо есть это могут быть поля, которые записаны по разному, но относятся к одному и тому же.\nПоэтому важно, если у нас категориальный признак, изучить нет ли повторящихся категорий, которые записаны немного по разному.\nТак как это создает шум, мы по сути имеем две разные категории, но на самом деле это одна. Нужно собрать их в одну.\n- И очень важно, если мы не подтвердили, что это действительно дубликат (например у нас нет ай ди клиента и мы не смогли выяснить один и тот же ли это человек),\nто нужно аккуратно удалять их. Но и оставлять много дублей плохо, так как они вносят шумы и искажения.\n- Помним, что наличие дубликата не говорит точно, что это дубль, возможно у нас нет ещё колонок, котоыре бы детализировали и разделили эти дубли.\nПоэтому тут могут быть рекомендации, чтобы добавли в фрейм доп колонки, которые помогут убрать дубли (либо сам ищешь ещё поля)\n\n\nКогда смотрим на дубли, то нужно ответить на вопрос так и должно быть, это нормально, что дубли в этих колонках.\nНапример у нас дубли во всех строках таблицы, нам нужно понять это может быть или этого не может быть, и нужно разбираться.\nАналогично когда смотрим колонки по 2, 3 и так далее, то самое главное, ответить на вопрос дубли могут быть в этих колонках.\nТакже когда разбиваем по категориям, задаем себе вопрос так могли распреедлеиться дубли.\n\n\ncheck_duplicated\ncheck_duplicated_combinations_gen\nВ первую функцию можно передавать весь датафрейм и можно выбирать нужные столбцы для проверки на дубли и передавать их.\n\n\nЕсли мы нашли колонки в которых дубликатов не должно быть, то нужно изучить эти дубликаты по категориальным переменным в нашем датафрейме\nanalys_by_category_gen\n\nНаблюдения: - Особых перекосов в сторону определенного значения в категории не наблюдается\n\nИдем по порядку с помощью next(gen)\nесли в выводе нет ничего интересного, то выше помещаем ячейку с таким содержимым %%capture\nnext(gen)\nснова выполняем next(gen), если снова ничего интересного то, к ячейке выше добавляем next(gen) будет так\n%%capture\nnext(gen);next(gen)\nи так далее, пока не появится важная ячейка\nдалее оставляем эту важную ячейку и снова повторяем с первого пункта,\nв итоге между ячейками с нужным выводом будут ячейки с запрещенным выводом и можно будет прогонять ноутбук весь целиком и выводы будут в нужнфх местах\n\n\nПосмотрим на дубли во всем датафрейме\n\n\n\nShow the code\npagri_data_tools.check_duplicated()\n\n\n\nПосмотрим сколько у нас дублей в каждой колонке\n\n\n\nShow the code\nseries_duplicated = pagri_data_tools.find_columns_with_duplicates(df)\n\n\n\nПосмотрим на строки датафрейма с дублями\n\n\n\nShow the code\ncol1_duplicated = series_duplicated['col1']\ncol2_duplicated = series_duplicated['col2']\n\n\n\n\nShow the code\ncol1_duplicated.head()\ncol2_duplicated.head()\n\n\n\n\nShow the code\npagri_data_tools.check_duplicated_combinations_gen()\npagri_data_tools.analys_by_category_gen()\n\n\n\nВажно на дубли проверить и отдельные строки и целиком таблицу и подумать какие группы столбцов могут дать дубли и на это тоже проверить.\n\n\nЕсли в дублях у нас есть ай ди клиента, то тут понятно, если нет ай ди, то пишем рекомендацию, чтобы данные приходили с ай ди,\nчтобы можно было понять это один человек или нет\n\n\nЕсли у нас id пользователя встречается не одни раз в таблице и есть другие поля которые должны быть всегда одни и те же,\nнапримем пол и прочее, то нужно проверить у всех ли пользователей все значения одинаковые в этом столбце.\nЭто может быть не только ай ди, любое уникальное поле, которое повторяется и для каждого этого поля есть другое\nполе, которое не должно меняться, нужно проверять а действительно ли это поле не меняется.\n\n\n\nИзучение пропусков\n\nНе забываем про ИИ.\nПишем название столбца (именно что они значат, то есть образование, пол и прочее), говорим, что есть пропуски.\nИ просим предложить причины этих пропусков. Если видим важное, то используем для рекомендаций, выводов и замены пропусков в предобработке.\n\n\nПроверяем на пропуски\n\n\nКогда смотрим на пропуски, то нужно ответить на вопрос так и должно быть, это нормально, что пропуски в этих колонках.\nКогда смотрим на пропуски по категориям, то думаем есть ли закономерность, не случайно ли распределение по категориям\n\n\nКогда мы встречаем пропуски, прежде всего, нужно ответить на вопрос, существует ли закономерность в появлении пропусков.\nИными словами, не случайно ли их возникновение в наборе данных.\nСлучайно, значит нет закономерности с соседними столбцами, то есть пропуски есть для разных значений.\nА могут быть неслучайные, то есть существует явная закономерностЬ, что пропуски есть только у сторок с общими занчениями в другом столбце.\nЧтобы это проверить, нужно взять столбец с пропусками, отфильтровать только пропуски (взять их) и\nпосмотреть как эти пропуски распределены по другой переменной.\n\n\nПервое что нужно сделать, когда мы видим пропуск или выброс, это проверить является ли оно случайным.\n&gt;То есть посмотреть не относятся ли все выбросы к одной категории. Если это так, то это уже не случайно и мы нашли аномалию, которую можно изучать.\n&gt;Если у нас случайны разброс пропусков в категориях, то значит тут есть случайность.\n&gt;Например, у нас возраст 0, и мы видим, что больше всего это у женщин. Следовательно получаем гипотезу, что женщины не хотят сообщать свой возраст.\n\nВ пропусках мы можем определить какие категории, платформы и прочее не собираются данные. Смотрим пропуски, далее смотрим у каких категорий их больше,\n&gt;и получаем вывод, что нужно обратить внимание на эти категории или системы, почему там пропуски\n\n\nfind_columns_with_missing_values\ncheck_na_in_both_columns\nanalys_by_category_gen\n\n\n\nShow the code\nseries_missed = pagri_prep.find_columns_with_missing_values(df)\n\n\n\n\n\n\n\nTable 2: Missings\n\n\n\n\n\ndays_employed\n2174 (10.10%)\n\n\ntotal_income\n2174 (10.10%)\n\n\n\n\n\n\n\n\n\nПосмотрим на строки датафрейма с пропусками\n\n\n\nShow the code\ncol1_missed = series_missed['col1']\ncol2_missed = series_missed['col2']\n\n\n\n\nShow the code\ncol1_missed.head()\ncol2_missed.head()\n\n\n\nПосмотрим сколько пропусков в обоих колонках вместе\n\n\n\nShow the code\npagri_data_tools.check_na_in_both_columns()\n\n\n\nИзучаем пропуски по категориям\n\n\nСмотрим на все поля\nin_category_pct говорит о том сколько в этом значении категории изучаемых значений\nin_column_pct говорит о том сколько процентов изучаемого значения данного значения категории в общем\ntotal_count_pct помогает анализировать in_column_pct, так как мы видим сколько занимает это значение в общем\n\n\n\nShow the code\ngen = pagri_prep.analys_by_category_gen(df, series_missed, is_dash=True)\n\n\n\n\nShow the code\nnext(gen)\n\n\n\n\nShow the code\ngen = pagri_prep.analys_by_category_gen(df, series_missed, is_dash=True)\n\n\n\n\nShow the code\ndf.head()\n\n\n\n\n\n\n\n\n\nchildren\ndays_employed\ndob_years\neducation\neducation_id\nfamily_status\nfamily_status_id\ngender\nincome_type\ndebt\ntotal_income\npurpose\n\n\n\n\n0\n1\n-8437.673028\n42\nвысшее\n0\nженат / замужем\n0\nF\nсотрудник\n0\n253875.639453\nпокупка жилья\n\n\n1\n1\n-4024.803754\n36\nсреднее\n1\nженат / замужем\n0\nF\nсотрудник\n0\n112080.014102\nприобретение автомобиля\n\n\n2\n0\n-5623.422610\n33\nСреднее\n1\nженат / замужем\n0\nM\nсотрудник\n0\n145885.952297\nпокупка жилья\n\n\n3\n3\n-4124.747207\n32\nсреднее\n1\nженат / замужем\n0\nM\nсотрудник\n0\n267628.550329\nдополнительное образование\n\n\n4\n0\n340266.072047\n53\nсреднее\n1\nгражданский брак\n1\nF\nпенсионер\n0\n158616.077870\nсыграть свадьбу\n\n\n\n\n\n\n\n\n\nИзучение выбросов\n\nНе забываем про ИИ.\nПишем название столбца (именно что они значат, то есть образование, пол и прочее), говорим, что есть выбросы.\nОбязательно приводим значения выбросов, самые характерные, чтобы дать ИИ болше информации. И просим предложить причины этих выбросов. Если видим важное, то используем для рекомендаций, выводов и замены выбросов в предобработке.\n\n\nКогда смотрим на выбросы, то нужно ответить на вопрос так и должно быть, это нормально, что выбросы в этих колонках.\n\n\nВыбросы это не только просто сильно большое или сильно маленькое значение.\n\nВыбросы нужно также смотреть по мультипараметрам, с помощью моделей и искать аномалии.\n\nВыброс это то, что отделяется от других, что выбивается из общей картины. Следовательно это что-то особенное.\n\nТажке выбросы говорят не только о плюсах, но и о минусах. Выбросы могут сказать, что у нас что-то сломалось.\n&gt;Что-то не записывается, или работает с багами. Все это можно увдитеь по выбрасам и аномалиям.\n\nОбязательно посмотреть выбросы в разрезе категорий, так как мы сможем сделать выводы об их источнике.\n\nЕсли мы работаем со строгой отчетностью, то тут любой выброс это уже инсайт и нужно идти разбираться откуда это взялось.\n\n\nЕсли мы во время изучения данных выявили потенциальные выбросы, то нужно их отдельно изучить.\nДля этого создаем датафрейм с нужными значениями и помещаем его в Series,\nиндекс это название колонки, в которой мы изучаем выброс.\nДалее отдаем этот Series в функцию analys_by_category_gen.\n\n\nСначала изучим потенциальные выбросы, которые мы выявили при изучении колонок.\nУ нас в количестве детей есть значение 20.\nИзучим его подробнее.\n\n\nВажно при изучении колонок записывать выбросы отдельно и потом коппировать сюда.\nА тут нужно изучить эти значения отдельно.\n\n\n\nShow the code\noutliers_series = pd.Series([df[df.children == 20]], index=['children'])\ngen = pagri_data_tools.analys_by_category_gen(df, outliers_series)\nnext(gen)\n\n\n\nСмотрим на выбросы используя Z-score\ndetect_outliers_Zscore\n\n\n\nShow the code\nseries_outliers = pagri_data_tools.detect_outliers_Zscore()\n# сначала смотрим на значения с большим количеством выбросов\nseries_outliers['col'].col.value_counts().to_frame('outliers')\n# затем уже изучаем определенные датафреймы в series_outliers\nseries_outliers['col'].head()\n\n\n\nСмотрим на выбросы используя квантили\ndetect_outliers_quantile\n\n\n\nShow the code\nseries_outliers = pagri_data_tools.detect_outliers_quantile(df)\n\n\n\nПосмотрим на строки датафрейма с пропусками\n\n\n\nShow the code\ncol1_outliers = series_outliers['col1']\ncol2_outliers = series_outliers['col2']\n\n\n\n\nShow the code\ncol1_outliers.head()\ncol2_outliers.head()\n\n\n\nИзучить выбросы по категориями\nanalys_by_category_gen\n\n\nСмотрим на все поля\nin_category_pct говорит о том сколько в этом значении категории изучаемых значений\nin_column_pct говорит о том сколько процентов изучаемого значения данного значения категории в общем\ntotal_count_pct помогает анализировать in_column_pct, так как мы видим сколько занимает это значение в общем\n\n\n\nShow the code\ngen = pagri_data_tools.analys_by_category_gen(df, series_outliers)\nnext(gen)\n\n\n\n\nИзучение отрицательных значений\n\nНе забываем про ИИ.\nПишем название столбца (именно что они значат, то есть образование, пол и прочее), говорим, что есть отрицательные значения там где их быть не должно.\nОбязательно приводим значения, самые характерные, чтобы дать ИИ болше информации. И просим предложить причины этих отрицательных значений. Если видим важное, то используем для рекомендаций, выводов и замены отрицательных значений в предобработке.\n\n\nОчень важно, если у нас есть столбец, в котором не должно быть отрицательных значений, то нам нужно отдельно изучить положительные и отрицательные значения.\nИ те и те посмотреть по категориям.\nИ на основе этого изучения мы моежм заметить причины отрицательных значений.\nНапример, в колонке стажа у нас очень много отрицательных значений и есть положительные значения.\nМы отдельно посмотрели отрицательные значения и они в основном принадлежат работающим людям.\nА положительные пренадлежат пенсионерам.\nВажно и полоительные и отрицательные значения посмотреть их макс и мин.\nВот мы для стажа посмотрели макси и мин и видим, что отрицательные значения похожи на реальные значения в годах.\nА вот положительные слишком большие, и далее мы поняли, что это данные в часах.\nВ итоге у нас уже много предположений, которые помогут выяснить откуда появляются странные данные в этом столбце.\nК тому же мы можем попробовать заменить отрицательные значения, если у нас есть уверенность на основе анализа.\n\n\nИзучаем отрицательные значения\n\n\n\nShow the code\nseries_negative = pagri_data_tools.find_columns_with_negative_values(df)\n\n\n\nОпределяем в каких колонках не должно быть орицательных значений.\nКолонки в которых допустимы отрицательные значения удаляем из series_negative\n\n\nИзучим отрицательные значения в разрезе категорий\n\n\nСмотрим на все поля\nin_category_pct говорит о том сколько в этом значении категории изучаемых значений\nin_column_pct говорит о том сколько процентов изучаемого значения данного значения категории в общем\ntotal_count_pct помогает анализировать in_column_pct, так как мы видим сколько занимает это значение в общем\n\n\n\nShow the code\ngen = pagri_data_tools.analys_by_category_gen(df, series_negative)\nnext(gen)\n\n\n\n\nИзучение нулевых значений\n\nНе забываем про ИИ.\nПишем название столбца (именно что они значат, то есть образование, пол и прочее), говорим, что есть нули там, гед их быть не должно.\nИ просим предложить причины этих нулей. Если видим важное, то используем для рекомендаций, выводов и замены нулей в предобработке.\n\n\nИзучаем нулевые значения\n\n\n\nShow the code\nseries_zeros = pagri_data_tools.find_columns_with_zeros_values(df)\n\n\n\nОпределяем в каких колонках не должно быть нулевых значений.\nКолонки в которых допустимы нулевые значения удаляем из series_negative\n\n\n\nShow the code\nseries_zeros = series_zeros.drop('children')\nseries_zeros\n\n\ndob_years           children  days_employed  dob_years educ...\ndtype: object\n\n\n\nИзучим нулевые значения в разрезе категорий\n\n\nСмотрим на все поля\nin_category_pct говорит о том сколько в этом значении категории изучаемых значений\nin_column_pct говорит о том сколько процентов изучаемого значения данного значения категории в общем\ntotal_count_pct помогает анализировать in_column_pct, так как мы видим сколько занимает это значение в общем\n\n\n\nShow the code\ngen = pagri_data_tools.analys_by_category_gen(df, series_zeros)\nnext(gen)\n\n\n\nПосле изучения данных у нас могут возникнуть вопросы по определенным значениям, это возможно не выбросы,\nмы просто хотим подробнее их изучить.\nДля этого создаем датафрейм с нужными значениями и помещаем его в Series,\nиндекс это название колонки, в которой мы изучаем выброс.\nДалее отдаем этот Series в функцию analys_by_category_gen.\nНужно сделать специальную функцию для этого, чтобы не использовать analys_by_category_gen\n\n\n\nShow the code\ncheck_series = pd.Series([df[df.col_for_check == value_for_check]], index=['col_for_check'])\ngen = pagri_data_tools.analys_by_category_gen(df, check_series)\nnext(gen)\n\n\n\nТакже мы можем изучить любые столбцы (или часть столбцов) по категориям.\nТо есть мы изучаем как распределены элементы по категориям\n\n\n\nShow the code\ncheck_series = pd.Series([df[df.col_for_check == value_for_check]], index=['col_for_check'])\ngen = pagri_data_tools.analys_by_category_gen(df, check_series)\nnext(gen)\n\n\n\nСделать функцию определения выбросов на основе машинного обучения\n\n\nДополнительные моменты, которые стоит проверить и изучить - Проверить на сложные выбросы, типа у нас есть статус и возраст и мы видим что студент имеет возраст 60 лет, это реально, но уже подозрительно.\nВот таких моментов может быть много, но нужно додуматься, чтобы найти такие комбинации, но это важно делать. - важно проверить на корректность данные, то есть смотрим по отдельности каждый столбец и изучаем мин, макс, и другие параметры, и\nдумаем, это физически реально. И особенно, когда у нас несколько связаных параметров, нет ли между ними противоречия.\nНапример, у нас есть дата показа рекламы и есть дата создания рекламы, естественно создание должно быть раньше, это нужно проверить.\n- Проверяем данные ошибки\nОшибки которые не являются дублями, пропусками или выбросами.\nЭто сложно сделать, хотя бы заметить явные ошибки - Проверить на ошибки согласованности\nНапример, у нас пользователь с одним ай ди имеет разные имена. display(df.groupby('name')['age'].nunique()) - вообще нужно придумать разные проверки для колонок, особенно связанных. И провести эту проверку.\n\n\n\n\nПромежуточный вывод\n\nИз наблюдений собираем важные выводы\n\n\nПринимаем решение, как именно мы будем проводить обработку, почему именно так, *зафиксировать рекомендации.\nТо есть отвечаем на вопрос, что будем делать с выбросами, что будем делать с null.\nБудет идеально если тут зафиксировать рекомендации\n\nПромежуточный вывод &gt; - children Присутствует 47 отрицательных значений с “-1”, а также аномалия в виде 20 детей … - days_employed Большая часть данных стобца со знаком “-”. Однако, эти данные представляют из себя 84% всей выборки. … будут заменены на .. исходя из определенного критерия, который будет описан далее. &gt; - Причины пропущенных значений в столбцах days_employed и income: &gt; - Во-первых, это может быть из-за неправильной выгрузки данных. Оставим это предположение до того момента, пока не убедимся в неверности других предположений.Наиболее вероятно &gt; - Во-вторых, одной из гипотез было предположение об отсутствии трудового опыта у данной части выборки. Однако, если распределение по возрасту в данной группе равномерное по всем возрастам выборки. Также большая доля этой части выборки трудоустроена. Гипотеза не подтверждена &gt; - В-третьих, возможно, что эта часть выборки не имеет официального трудоустройства. Данная гипотеза вызывает сомнение в связи с тем, что при наличии достаточно большого стажа работы у представителей выборки у ее представителей нет официального трудового стажа. К тому же 18.9% данной выборки являются госслужащими. Гипотез не подтверждена - age .. 0 возраст у 101 человека. - education & education_id Необходимо будет привести данную категорийнуй переменную к общему виду. Избавиться от разного регистра. Но можно не тратить на это время и использовать следующий столбец education_id. Это позволит использовать меньше памяти и не повлияет на качество анализа. - …"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#предобработка-данных",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#предобработка-данных",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Предобработка данных",
    "text": "Предобработка данных\n\nВажно, когда удаляем строки, то делаем сброс индекса\n\n\n\nShow the code\n.reset_index(drop=True)\n\n\n\nНе забываем про ИИ\nКогда мы проводим предобработку данных, то первый вопрос мы себе задать следующий Какава вероятнсоть, что это является истиной? Если вероятнсоть ниже 60 прцоентов, то это делать не стоти и может лучше оставить как есть или не трогать этот столбец.\nНапример, у нас дубли или отрицательные значеия и мы выдвинули гипотезу, что это просто неправильный знак и хотим взять модель числа.\nНо если мы подумаем, а высокая ли вероятность, что число просто с неверным знаком, то вероятнсот этого низкая.\nПоэтому это делать не нужно.\nДругое дело у нас датафрейме 1 прцоент полных дублей и при этом у нас есть достаточно точные колонки типа зарплаты с точностью до рублей или стаж в днях.\nВот тут мы можем с высокой вероятностью утверждать, что это дубли, так как мало вероятно что будет две записи настолько точно совпадать.\nПоэтому сначала думаем насколько вероятна та гипотеза, которую мы выдвинули и хотим по ней изменить наши данные.\nТут лучше придерживаться правила не навреди.\n\n\nКогда удаляем значения из категориальных столбцов pandas, и в этом столбце нет больше таких занчений, которые удалил.\nТо нужно удалить это значение из категории\n\n\n\nShow the code\ndf.drop(df[df.gender == 'XNA'].index, inplace=True)\ndf['gender'] = df['gender'].cat.remove_unused_categories()\ndf.value_counts(dropna=False)\n\n\n\n\nShow the code\ndf.shape[0]\n\n\n\nОбрезание неполных временных периодов\n\nЕсли у нас датасет за год, например, и первый или последний месяц неполные, то их лучше выбрасить, если мы будем\nрасчитывать месячные метрики.\nНо сначала конечно нужно проанализировать столбцы без обрезания, чтобы убедиться, что там нет ничего необычного.\n\n\n\nВыбор нужных столбцов для дальнейшей работы\n\nСохраним исходный датафрейм в переменную df_origin, чтобы была возможность вернуться к нему\n\n\n\nShow the code\ndf_origin = df.copy()\n\n\n\nУдаляем ненужные столбцы\n\n\n\nShow the code\ndf = df.drop(['col1', 'col2'], axis=1)\ndf.head(1)\n\n\n\nДумаем, какие колонки нам нужны, выбираем только их для дальнейшей работы.\n&gt;Остальные убираем в другой датасет.\nВажно после изученя данных сначала убрать не нужные столбцы, а потом уже заниматься преобразованием (удалением пропусков и выбросов).\n&gt;Думаем прежде чем удалять строки, так как возможно лучше удалить столбец и строки удалять будет не нужно.\n\nПишем почему выбираем определенные столбцы\n\n\n\nОбработка выбросов\n\nНе забываем про нулевые значения и отрицательный.\nВ столбцах, где их быть не должно, они являются выбросами.\n\n\nС обработкай нулевых и отрицательных значений нужно быть внимательным.\nНужно сначала хорошо подумать, откуда могло это появиться,\nтут поможет анализ этих значений в предыдущей главе.\nДумаем откуад появилось отрицательное или нулевое занчение,\nи если у нас есть гипотезы, которые похожи на правду (мы думаем что вероятность их истины больше 60%),\nто мы обрабатываем их исходя из гипотезы.\nНапример, -1 часто бывает как отсутсвие чего-то, то есть мы в зависимости от контекста можем заменить его на 0.\n\n\nВажно каждый раз, когда мы удаляем что-то из датафрейма, то убедиться, что мы удалили столько строк, сколько и хотели.\nДля этого выводим размер датафрейма до удаления.\nСмотрим сколько строк мы хотим удалить.\nДалее не сохраняя в датафрейм удаляем строки и смотрим верный ли итоговый размер.\nЕсли все верно, то удаляем уже с сохранением.\n\n\nНе забываем, что выбросы мы также можем заменять на медианные значения.\n\n\n\nShow the code\ndf.shape[0]\n\n\n21525\n\n\n\n\nShow the code\ndf = df[df.children &gt;= 0]\ndf.shape[0]\n\n\n21478\n\n\n\nПосмотрим где у нас отрицательные значения\n\n\n\nShow the code\npagri_data_tools.check_negative_value_in_df(df)\n\n\n\nПосмотрим где у нас нулевые значения\n\n\n\nShow the code\npagri_data_tools.check_zeros_value_in_df(df)\n\n\n\nОбрабатываем нулевые и отрицательные значения, затем снова проверяем\n\n\n\nShow the code\npagri_data_tools.check_negative_value_in_df(df)\n\n\n\n\n\n\n\n\n\nnegative\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npagri_data_tools.check_zeros_value_in_df(df)\n\n\n\n\n\n\n\n\n\nzeros\n\n\n\n\n\n\n\n\n\n\nТакже нужно обработать выбросы, которые мы обнаружили при изучении данных.\nЭто могут быть любые колонки со значениями, которые не моут быть в реальности.\n\n\nНужно сначала обработать выбросы, а потом уже обрабатываться пропуски.\nТак как мы заоплняем пропуски, учитывая значения в колонке, которые возможно мы потом удалим.\n\n\nПомним про нулевые и отрицательные значения\nНулевые значения, отрицательные значения являются выбросами, если они не могут быть у этой колонки.\n\nОчень важно понимать, когда выброс можно отбросить и он реально выброс и когда нельзя.\n&gt;Опираемся на физику параметра, думаем это значение физически возможно.\n\nТакже выброс может казаться выбрасом, но для бизнеса это не выброс.\n&gt;Например у нас суммы покупок и одна покупка сильно выделяется, а там просто человек купил супе дорогой каньяк, например.\n\nКогда хотим обрезать выбросы, то думаем, какой порог может быть физически реальным и по нему режем, а не просто так берем какой-то перцентиль.\n&gt;Всегда нужно думать с точки зрения физического возможного значения параметра и по нему резать (подумать а какое значение может быть максимально реальным и по нему обрезать)\nЕсли мы имеем дело со строгой отчестностью, то выбросы убирать нельзя, нужно разобраться откуда они.\n\nЕсли мы не можем с увереностью сказать, что это выброс, то нам не стоит его выкидывать, но работать как то нужно с ними,\n&gt;тогда, логарифмируем (лучше использовать натуральный логарифм) эту колонку и работаем с такими значениями (тогда выбросы сожмуться).\n\n\nПосле удаления выбрасов, можно снова выполнить пункт про изучение выбрасов, так как выбросы могут появиться новые,\nесли у нас например выбросы были слишком нереальные значения, когда мы от них избавимся, будет лучше видно другое\n\n\n\nОбработка пропусков\n\nВажно помнить, что пропус может быть вызван тем, что во измежании дублирования строк, при созаднии сводной таблицы,\nзанчения не повторяются, а если потом эту таблицу куда то отправить, то там эти пропуски могут стать null\nПоэтому сначала смотрим последовательно на значения и думаем, не может ли это быть таким случаем.\nЭто могут быть даты, которые идут подряд и меду ними пропуски.\nИли список названий четко по порядоку и между ними пропуски, это может быть вызвано как раз последствием создания сводной таблицы.\nВ пандас это видно, когда мы группируем строки, у нас в индексе дубли не пишуться, но пандас занчет, что там есть занчения,\nно после импорта куда-то там могут не продублироваться значения и возникнут пропуски.\n\n\nПрежде чем обрабатывать пропуски, нужно подумать а можем ли мы их заменить исходя из имеющихся столбцов.\nНапример, у нас есть столбец с пропусками возраст, и есть стаж,\nмы можем возраст заменить так стаж + 18 + 5\nАналогично другие ситуации нужно сообразить как можно заменить пропуски.\nИ только если нет идей, тогда уже заменяем на медиану, например, по группам.\n\n\n\nShow the code\npagri_data_tools.check_missed_value_in_df(df)\n\n\n\nЕсли решии заменять прпоуски значениями, учитывая категории, то нужно убедиться, что размер этих категорий достаточный.\n\n\nПосмотрим размеры групп, если заменять внутри этих групп\n\n\n\nShow the code\ncategory_columns = ['education', 'family_status', 'gender', 'income_type']\nvalue_column = 'total_income'\npagri_data_tools.check_group_count(df, category_columns, value_column)\n\n\n\nЗаполним пропуски в группах от 10 элементов\n\n\n\nShow the code\ndf[value_column] = pagri_data_tools.fill_na_with_function_by_categories(df, category_columns, value_column, func='median', minimal_group_size=10)\n\n\n\nПроверим сколько у нас осталось пропусков\n\n\n\nShow the code\npagri_data_tools.check_missed_value_in_df(df)\n\n\n\nЕсли пропуски остались, то убираем какую-нибудь категорию и повторяем.\n\n\nчто-то изменили - &gt; посмотрели не изменилось ли количество дублей\ncheck_duplicated\n\n\n\nShow the code\npagri_data_tools.check_duplicated()\n\n\n\nУвидели пропуск - подумайте, нормально ли это. Сколько вообще пропусков может быть в этом столбце?\nК примеру, в списке с электронными адресами пользователей, согласных на рассылку, будет много пропусков. Далеко не все предоставляют email.\n\n\nМожно использвоать такой подход - если количество пропусков меньше 5 процентов, то удаляем (лучше меньше 1 процента) - если количество пропусков от 5 до 20 процентов, то подбираем чем заменить, удалять не стоит - если больше 20 процентов, то не трогаем, так как исказим\n\n\nНо оставляя пропуски, нам нужно помнить, что мы не можем по этим полям считать корреляцию с другими,\nтак как пропуски испортят расчет коэффициента корреляции. Аналогично другие метрики могут считаться некорректно.\nПоэтому, если мы будем считать показатели по столбцу с пропусками, то их нужно либо убирать, либо этот столбец не использовать для расчетов.\n\n\nДля категориальных переменных оставлять пропуски нельзя, так как мы скорее всего будем группировать по ним и смотреть разные разрезы.\nПоэтому в худшем случае, если не можем ничем заменить, и нет уверености, что пропуск можно заполнить пустой строкой (если значения физически нет),\nто создаем категорию например other из пропусков.\n\n\nЕсли у нас пропуски в категориальной переменной и есть разные периоды или просто данные разбиты на части (то есть эта категориальная переменная повторяется),\nто мы можем взять ещё какую-нибудь переменную, у которой нет пропусков, где пропуски у первой переменной и далее посмотреть другие периоды\nТаким образом у нас будет предыдущий период, где будет занчение второй переменной и первой и если в нескольких периодах они одинаковые, то мы можем\nзаполнить и пропуски этим значением.\nЕщё раз схема такая - берем 2 поля одно с пропусками, другое без, получаем новую таблицу, в этой таблице оставляем только униклаьные значения в поле без пропусков,\nпо этому полю будем джойнить. Далее в основнйо таблице дропаем описание и создаем новое описание из таблицы справочника.\nfill_missing_values_using_helper_column\n\n\n\nShow the code\npagri_data_tools.fill_missing_values_using_helper_column()\n\n\n\nЗаполняем пропуски учитвая категории\nfill_na_with_function_by_categories\n\n\nВажно следить, чтобы категории, по которым будем заполнять пропуски были обработаны.\nЕсли у нас в категориальной переменной есть значение с большой буквы и с маленькой, то это одна категория,\nно замена будет идити по двум, чтобы такого не было, нужно сначала обработать категориальную переменную.\n\n\nТакже важно, чтобы в группах по которым мы будем считать значение для заополения было достаточно значений\nдля выбранной функции.\nНапример, если мы решили брать среднее, а в группе у нас 5 значений, то среди них может быть выброс и наше среднее будет некорректно.\nЛучше в такой ситуации брать группу побольше для этих микрогрупп.\nВ идеале группы должны быть от 30 элементов.\n\n\nМожно посмотреть какой процент группах без значений\n\n\n\nShow the code\ntemp = df.groupby(['education', 'family_status', 'gender', 'income_type'])['total_income'].sum()\n(temp == 0).sum() * 100 / temp.size\n\n\n\n\nShow the code\npagri_data_tools.fill_na_with_function_by_categories()\n\n\n\nСделать функцию заполнения пропусков с помощью машинного обучения\n\n\nПосле удаления пропусков и выбросов желательно проверить какой прцоент строк мы удалили.\n\n\n\nОбработка дубликатов\n\nВсе значения в колонках во всех таблицах нужно привести к нижнему регистру и по возможности к одному языку,\nдля перевода к одному языку можно использовать словарь, с помощью которого изменить неправильный язык\nЭто нужно, чтобы когда будем соединять таблицы, у нас условие соеденения правильно сравнивало равные значения.\n\n\nМожно посмотреть снвоа на дубликаты после обработки пропусков.\n\n\ncheck_duplicated\nfind_columns_with_duplicates\ncheck_duplicated_combinations_gen\nget_duplicates_value_proportion_by_category\nВ первую функцию можно передавать весь датафрейм и можно выбирать нужные столбцы для проверки на дубли и передавать их.\n\n\n\nShow the code\npagri_data_tools.check_duplicated()\n\n\n\n\nShow the code\npagri_data_tools.check_duplicated_value_in_df(df)\n\n\n\n\nShow the code\npagri_data_tools.find_columns_with_duplicates()\n\n\n\nЗаполним пропуски в группах от 10 элементов\n\n\n\nShow the code\ndf[value_column] = pagri_data_tools.fill_na_with_function_by_categories(df, category_columns, value_column, func='median', minimal_group_size=10)\n\n\n\n\nShow the code\npagri_data_tools.check_missed_value_in_df(df)\n\n\n\n\n\n\n\n\n\nmissed\n\n\n\n\ntotal_income\n63 (0.3%)\n\n\n\n\n\n\n\n\nЕсли есть дубли, и мы считаем, что это не дубли, а просто разделились данные,\nто объединеняем записи, которые имеют одинаковые значения ключевых признаков.\nmerge_duplicates\n\n\n\nShow the code\npagri_data_tools.merge_duplicates()\n\n\n\nЕсли мы не уверены, что дубль является дублем и не хотим удалять, то можно использовать\nмаркировку дублей, можно добавить новую колонку, которая будет содержать информацию о том,\nявляется ли строка дубликатом или нет.\ndf['is_duplicate'] = df.duplicated()\n\n\n\nShow the code\ndf['is_duplicate'] = df.duplicated()\n\n\n\nПодумать, а можем ли мы обогатить данные, что разделит дубли.\nТо есть возможно в наших данных нет какого-то столбца, и тогда дубли уже не будут дублями.\n\n\nЕсли уверены, что это дубли, то удаляем их\ndf.drop_duplicates()\n\n\n\nShow the code\ndf.drop_duplicates()\n\n\n\n\nПриведение данных к удобной форме\n\nИзменяем значения в столбцах на более удобные\n\n\n\nShow the code\ndf.is_apartment.value_counts(dropna=False)\napp = df.is_apartment.apply(lambda x: 'да' if isinstance(x, bool) else 'нет').astype('category')\napp.value_counts(dropna=False)\n# df.debt = df.debt.apply(lambda x: 'есть' if x == '1' else 'нет').astype('category')\n\n\n\nЕсли у нас в столбце, например, стаж данные в днях, то это нужно преобразовать в года.\nТакже если у нас в других столбцах данные в формате, который нужно изменить для лучшего анализа, то делаем это.\n\n\nОкруглим значения в поле дохода до целого.\nЦелая часть выглядит реальной. А с дробной частью нужно разбираться почему стоько знаков.\n\n\n\nShow the code\ndf.total_income = df.total_income.round().astype('int32')\ndf.head(1)\n\n\n\n\n\n\n\n\n\nchildren\nage\neducation\nfamily_status\ngender\nincome_type\ndebt\ntotal_income\npurpose\n\n\n\n\n0\n1\n42\nвысшее\nженат / замужем\nF\nсотрудник\n0\n253876\nпокупка жилья\n\n\n\n\n\n\n\n\nПосмотрим сколько у нас людей с полом XNA осталось\n\n\n\nShow the code\n(df.gender == 'XNA').sum()\n\n\n1\n\n\n\nПосмотрим кто это\n\n\n\nShow the code\ndf[df.gender == 'XNA']\n\n\n\n\n\n\n\n\n\nchildren\nage\neducation\nfamily_status\ngender\nincome_type\ndebt\ntotal_income\npurpose\n\n\n\n\n10701\n0\n24\nнеоконченное высшее\nгражданский брак\nXNA\nкомпаньон\n0\n203905\nпокупка недвижимости\n\n\n\n\n\n\n\n\nВсего 1 человек. И мы не можем идентифицировать его пол.\nУдалим, чтобы не мешало анализировать графики.\n\n\n\nShow the code\ndf.shape[0]\n\n\n21402\n\n\n\n\nShow the code\ndf.drop(df[df.gender == 'XNA'].index, inplace=True)\ndf['gender'] = df['gender'].cat.remove_unused_categories()\ndf.value_counts(dropna=False)\n\n\n21401\n\n\n\n\nShow the code\ndf.shape[0]\n\n\n\n\nКатегоризация данных и создание новых переменных\n\nЕсли у нас есть категориальная переменная, в которйо больше 3 значений, то нужно подумать а не можем ли мы из нее сделать\nновую категориальную переменную с 2-3 значениями, но тут важно, чтобы это несло смысл. Тут нам может помочь ИИ. И сообразительнсоть. Часто сразу не заментны возможные категории, котоыре несут смысл.\nТут исходим из смысла, наша задача созадть перменную, которая добавит нашему исследованию новый смысл, даст как бы новый разрез, и это улучшит\nкачество наших выводов.\nНапример, у нас столбец семейный стату, и там 6-7 статусов, мы можем собрать их в 2 семейный статус и не семейный статус.\nТут отлично помогает ИИ. Пишешь ему название переменной, униальные значения в ней,\nи просишь придумать возможную новую категориальнуюд переменную из 2-3 значений.\n\n\nВообще при категоризации ИИ очень хорошо помогает, он может дать идеи возможных категорий на оснвое имеющихся значений.\nПоэтому можно все столбцы прогонять через ИИ и смотреть что он предлагает, если есть то , что даст новый разрез нашим данным, то созадем категорийю.\n\n\nВажно, когда мы создаем категории, то всегда смотреть value_counts.\nИ делаем так, чтобы в каждой группе было достаточно элементов, хотя бы больше 30, а лучше больше 100.\nИначе выводы будут некоректные.\nВ идеале, чтобы количество элементов в каждой группе было от 1000. Лучше изменить диапазон и забрать часть данных от другой категории.\n\n\nВажно, когда создаем категориальную переменную, то даем ей тип category\nЧтобы она появилась на графиках (так как идет фильтрация на числовые и категориальные)\nи чтобы места меньше занимала\n\n\nПридумываем какие колонки можно дополнительно сделать из имеющихся.\nНапример у нас есть колонка длительность звонков, и 0 это пропущенный звонок,\nмы можем сделать колонку is_missed, в которой будет true или false\n\n\nСтараемся сделать категориальную колонку с да нет для всех возможных колонок.\nНапример, у нас колонка количество детей и есть 0, 1, 2, 3, 4, 5 мы созадем колнку\nесть дети или нет. 2 значения\nЭто очень полезно, так как мы можем посмотреть это на графиках и проверить гипотезы\nстат тестами.\n\n\nСмотрим на колонки и думаем можно ли из нее сделать колонку с 2 значениями,\nнапример есть и нет что-то\n\n\nОчень важно, когда мы создаем новые колонки, в которых используем несколько дургих, то нужно проверить распределение этой новой переменной, особенно выбросы.\nНапример, у нас начальная и конечная дата сессии и мы считаем длительность сессии. Вот тут нужно посмотреть какая минимальная длительность\nи какая максимальная. Ну и естественно проверить есть ли длительность 0 и меньше нуля.\nТаким образом мы можем найти инсайты уже после создания новых колонок, хотя в изначальных данных этих инсайдов не было видно.\n\n\nОбычная категоризация данных\n\n\nКатегоризация помогает избежать проблемы с разреженными данными, когда у нас есть слишком много групп с небольшим количеством элементов.\nЭто может привести к некорректным выводам и ошибкам в анализе. Категоризация нужна, чтобы образовать группы, в которых достаточно значений для использования статистических методов.\nИ вообще, если в группе 1-10 элементов, например у нас возраст пользователей и 5 человек с возрастом 22, 3 человека с возрастом 23 и так далее.\nМы не можем разбивать по таким группам, так как их размер небльшой и выводы будут некорректные, поэтому нам нужно собрать их в группы,\nчтобы у нас были группы с достаточным размером.\n\n\nЕсли у нас категориальная переменная имеет много значений, то мы не можем номрально с ней работать.\n&gt;Так как мы не можем построить графики по ним, так как их много и они не числовые. Не можем сравнить их все.\n&gt;Поэтому нам нужно сократить категории.\n\nНужно посмотреть на данные и подумать можем ли мы разделить их по сегментам рынка или по другим категориям, которые нам помогут.\n\nМы можем категоризировать на основе и числовых и категориальных столбцов. То есть мы можем из категориальной переменной сделать\n&gt;другую категориальную, уменьшив или увеличив разбиение.\n\nдобавление категорий обогощает данные, при чем категории могут формироваться не из одной колонки, а из серии, то есть чтобы попасть\n&gt;в определенную категорию значения столбцов должно быть такое то, а не только один столбец определяет категорию.\n\nкатегории могут быть да нет, то есть состоять из двух значений, например, у нас есть данные о рекламе и столбец где она показвалась,\n&gt;и у нас много много разных устройств. Мы можем разбить на да нет, то есть показвалась реклама по телеку или нет\n\n\nМы можем разбить данные на категории двумя способами - разбивать на равные части\nподходит, когда - диапазон значений является равномерным и имеет линейную структуру\n- мы понимаем на какие интервалы хотим разбить данные\n- мы хотим разделить диапазон значений на равные части для удобства анализа. - разбить на основе квантилей\nподходит, если\n- диапазон значений имеет неравномерную структуру - мы не можем понять какие интервалы выбрать - хотим выделить группы с конкретными характеристиками (например, группы с низким доходом, средним доходом и высоким доходом)\n\n\nВыбираем нужные способ и используем\ncreate_category_column\n\n\nЧтобы посмотреть распределение по квантилям используем pagri_data_tools.quantiles_columns()\n\n\n\nShow the code\npagri_data_tools.quantiles_columns()\n\n\n\n\nShow the code\npagri_data_tools.create_category_column()\n\n\n\nСделаем следующие группы - до 30 лет - от 30 до 40 лет - от 40 до 50 лет - от 50 до 60 лет - старше 60 лет\n\n\n\nShow the code\nlabels = ['до 30', '30-40', '40-50', '50-60', 'старше 60']\nbins = [-np.inf, 30, 40, 50, 60, np.inf]\n\n\n\n\nShow the code\ndf['age_cat'] = pagri_data_tools.create_category_column(df.age, labels=labels, bins=bins)\ndf['age_cat'].value_counts()\n\n\n30-40        5704\n40-50        5241\n50-60        4520\nдо 30        3804\nстарше 60    2132\nName: age_cat, dtype: int64\n\n\n\nКатегоризация с использованием лемматизации\n\n\nЕсли у нас есть столбец и мы хотим его лематизировать, то используем функцию\nlemmatize_column\n\n\nЧтобы создать лемы для словаря категоризации, можно посмотреть имеющиеся предложения и использовать\nm = Mystem()\nm.lemmatize('образованием')\n\n\n\nShow the code\nm = Mystem()\nm.lemmatize('образованием')\n\n\n\n\nShow the code\npagri_data_tools.lemmatize_column()\n\n\n\n\nShow the code\ncategorization_dict = {\n    'недвижимость': ['жилье', 'недвижимость']\n    , 'образование': ['образование']\n    , 'автомобиль': ['автомобиль', 'машина']\n    , 'свадьба': ['свадьба']\n}\n\n\n\n\nShow the code\ndf['purpose_new'] = pagri_data_tools.categorize_column_by_lemmatize(df.purpose, categorization_dict, use_cache=True)\ndf['purpose_new'].value_counts()\n\n\nнедвижимость    10779\nавтомобиль       4288\nобразование      3997\nсвадьба          2337\nName: purpose_new, dtype: int64\n\n\n\nЕсли нужно, уддалим старую колонку\n\n\n\nShow the code\ndf = df.drop('purpose', axis=1).rename(columns={'purpose_new': 'purpose'})\ndf.head(1)\n\n\n\n\n\n\n\n\n\nchildren\ndob_years\neducation\nfamily_status\ngender\nincome_type\ndebt\ntotal_income\npurpose\ndob_cat\ntotal_income_cat\n\n\n\n\n0\n1\n42\nвысшее\nженат / замужем\nF\nсотрудник\n0\n253876\nНедвижимость\n40-50\n200-500 тыс\n\n\n\n\n\n\n\n\nС помощью лематизации мы можем сократить количество категорий.\n\n\nНапример мы можем выделить группы: - операции с автомобилем (ключевое слово - автомобиль) - операции с недвижимостью (ключевые слова: жилье, недвижимость) - проведение свадьбы (ключевое слово: свадьба) - получение образования (ключевое слово: образование)\n\n\nИспользуем функцию\ncategorize_column_by_lemmatize\n\n\n\nShow the code\npagri_data_tools.categorize_column_by_lemmatize()\n\n\nИз времени также можно сделать категориальный переменные, например, создать переменную для времени заправки, если больше 1 минуты, то долгая заправка, иначе короткая и так далее.\nВАЖНО подумать какие переменные мы можем создать не только категориальные, но и числовые и временные.\nНапример, обрезание времени, чтобы получить дату по часам и прочее,\n\nЕсли мы хотим преобразовать категории в числа, то мы можем использовать - lable encoding\nЗаменяем быквы числами. Хорошо работает, когда у нас порядковые категориальные переменные.\nНе забываем про порядок, если у нас алфавитный порядок наших категорий соотвествует числовому, то ок,\nесли нет, то нам нужно самим определить порядок чисел, чтобы они соответствовали категориям в нужном порядке.\n- one hot encoding\nЕсли у нас категориальная переменная не упорядочиваемая, то лучше использовать one hot encoding, чтобы разница между числами не вносила шум,\nтак как черный и белый и красный цвет закодированные 1, 2, 3 вносят смысл количества, но они не имеют этого свойства.\n- target encoding\nзамена категориальной переменной на каую-то статистику по одной из категорий внутри этой переменной.\nНапример у нас категориальная переменная это наличие задержки. Значение задержан / незадержан. Мы кодируем их как 0 и 1. Далее мы берем и считаем по каждой группе (для задержан и для незадержан)\nстатистику, например, среднее и получаем столбец, где вместо каждой буквы будет ее среднее.\nТут важно делать регуляризацию. Так как маленькие группы могут иметь сильно зашумленные статистики, так как если у нас\nгруппа из 5 значений, то среди них может быть легко экстремальное одно и оно сбивает статистику, поэтому добавляем штраф всем статистикам.\nРегуляризация это что-то похожее на сглаживание.\nКак это делается - берем считаем среднее по таргету (целевой переменной, то есть той, по которой мы счтаем статистику) всей таблице (то есть не делим на категории)\n- Далее используем следующую формулу для сглаженного значения среднего по конкретной группе:\n(среднее по группе * количество элементов в группе + среднее по таргету без учета категорий * размер регуляризирующей группы) / (количество элементов в категории + размер регуляризирующей группы)\nКоличество элементов в регуляризационнной группе выбирает эмперически. То есть это количество элементов, которым мы сглаживаем.\nСмысл в том, что мы берем сколько-то элементов с занчением для всех категорий и сглаживаем им наши отдельные категории.\n- Размер регуляризирующей группы обычно выбирают с помощью grid search, то есть берут цикл для размера этой группы и считают результат модели для каждого размера,\nи потом выбирают тот размер, для которого результат лучше.\ntarget_encoding_linear\ntarget_encoding_bayes\n\n\n\nShow the code\npagri_data_tools.target_encoding_linear()\npagri_data_tools.target_encoding_bayes()\n\n\n\nИспользование кластеризации для категоризации\n\n\nМожно понизить размерность до 3\nи построить 3 д график\nПо этому графику посмотреть есть ли у нас возможные кластеры\nЕсли есть, то выделить их\nПричем для понижения размерности можно брать все столбцы, а можно только часть.\n\n\n\nОбогощение таблиц\n\nПроверка соответствия:\nЕсли у нас в разных таблицах есть значения, которые дожны быть одинакоые,\nто нужно проверить, что значения в одном столбце соответствуют значениям в другом столбце.\n\n\n\nShow the code\ndf['column_name1'].equals(df['column_name2'])\n\n\n\nОбоготить данные можно следующими способами - взять поле нашей таблицы и найти дополнительные данные в интернете или ещё где-то и потом связать с нашей колонкой по этому полю\nСамое просто это дата, если у нас есть дата, то мы можем много разной доп информации внести в наши данные связывая по дате.\nТакже, например, у нас есть какие-то коды чего-то, мы ищем информацию по этим кодам и находим табличку с доп инфой по этим кодам и можем обоготить ими\nнашу таблицу. Например, у нас города или страны, мы можем по ним также внести доп инфу из какого-то источника, которая нам поможет.\nВообще любое поле нашей таблицы это потенцильная нить для обогощения. Главное понять с чем полезным мы можем соеденить\nчерез конкретное поле, чтобы получить больше полезной информации для анализа, по сути для детализации наших зависимостей или для поиска\nновых зависимостей и инсайтов в них.\nПроцесс следующий - мы берем каждую колонку нашего дата сета и думаем, с чем через нее мы можем связать и если придумываем, то идешь ищем эту информацию и\nв итоге соединяем.\n- Можно пойти от обратного. Сначал подумтаь какие данные нам могут помочь и поискать их в интернете например, а потом уже думать как их соеденить с нашими\nданными. Оба способа лучше делать одновременно.\n\n\nКаждый раз, когда мы работаем с дата сетом, мы должны понять что является сущностью этого дата сета.\nНапример событие, человек и прочее.\nДалее нам нужно поянть а можем ли мы его идентифицировать по текущим данным (не всегда есть уникальный ай ди).\nЕсли не можем, то нужно думта как обогатить данные, чтобы четко идентифицировать сущности\n\n\nЧто нужно обязательно првоерить после соединения - если мы соединяем по полю, которое уникально в обеих таблицах - количество строк в левом датафрейме равно количеству строк в итоговом - параметры каждого дата сета не изменились (если мы соединили правильно, то итоговые суммы по столбцам не должны измениться) - используем df.sum(numeric_only=True) для каждой таблицы до соединения и для общей таблицы и сравниваем значения - можно использвоать df.describe также до и после объединения и сравнивать параметры - если у нас в одной из колонок для соединения не уникальные значения (то есть для одной строки в левой таблице будет несколько в итоговй)\n- Сначала группируем таблицы, чтобы поле для соединения в обеих таблицах было уникальное и применяем предыдущий шаг с количеством строк в левой и итоговой и суммой значений в левой и итоговой одинаковой - Если нам нужно соеденить без группировки (но это редко может быть, поэтому нужно подумать точно ли не моежм сгруппировать)\nтогда нет выбора и остаются только следующие варианты\n- если в левой таблице уникальные записи в колонке, по которйо соединяем\n- тогда считаем сколько было записей в левой таблице в колонке для соединения и сравниваем с количеством уникальных записей в итоговой\nони должны совпадать, но тут важно в итоговой брать уникальные записи - есил и в левой и правой нет уникальных - тут считаем сколько уникальных в левой до и сколько уникальных в итоговой, должно совпадать\n\n\nЕсли у нас что-то не сходится после соединения таблиц, то нужно внимально изучить это.\nТут может быть инсайт (кто-то не правильно вносит информацию, какие-то значения неверные или кто-то что-то хотел спрятать, не указать и прчоее).\nКогда видим нестыковки после соединения таблиц, то должна загораться красная лампочка. Это потенциальный инсайт, баг, который мы можем найти и сообщить, чтобы его починили.\n\n\nпомним, что метод соединения inner стоит по умолчанию в merge\n\n\nВ колонках, по которым будем соеднить, проверяем, нет ли пропусков, пропуски нужно заменить нулями.\nИначе будет либо ошибка, либо пропуски сджойнятся с пропусками\n\n\nПроблема справочников\nПри объединение таблиц важно помнить про то, что в разных таблицах не только названия столбцов может быть разное,\nно и одно значение может быть записано по разному в разных таблицах, например, названия профессий, названия городов,\nимя в одной таблице на русском, а в другой на английском, номер телефона с черточкой или плюсом и без черточки или плюса.\nПоэтому не забываем привести все значения таблиц к нижнему регистру, чтобы не было проблем разными регистрами для одного слова\n\n\nПроблема временных зон\nВ одной таблице может быть выгрузка по местному времени, а в другом по московскому\n\n\nПроблема курсов валют\nРазыне системы могут брать курс за разные промежутки вермени, например, одна система берет курс в гугле (раз в час обновляется),\nа другая система берет курс в ЦБ (обновляется раз в сутки)\nИ поэтому итоговые резултаты могут не состыковаться, поэтому, когда видим курсы валют, то нужно убедиться. что они взяты из одного испточника\nи за один промежуток времени\n\n\nКогда мы работаем с данными, нам важно четко идентифицировать клиентов, событие или другую сущность, с которой мы работаем.\nИначе у нас будет шум, так как мы одного и того же клиента учтем более одного раза.\n\n\nКак можно обоготить данные, чтобы лучше идентифицировать сущности - Добавить для клиента email, телефон, устройство, 4 цифры карты и другое, что может помочь его идентифицировать\nЭто важно так как у клиента могут быть разные телефоны, устройства, карты, но все это вместе поможет его идентифицировать точнее - Добавить для события локацию, погоду, связанные событие, праздники, что поможет нам идентифицировать событие\n\n\n\nПромежуточный вывод\n\nпишем как обработали данные, например\n\n\nУдалили колонки с id образования и семейного статуса, так как нам для графиков лучше подойдут названия, а не id.\nКолонка со стажем имеет совершенно некорректные данные. Чтобы не внести искажение в анализ, удалим эту колонку.\nУдалили отрицательные значения в колонке с количеством детей, которые составляли 0,2% от общего количества записей в данных."
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#расчет-метрик",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#расчет-метрик",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Расчет метрик",
    "text": "Расчет метрик\n\nМетрики продукта\n\nтут будут расчеты продуктовых метрик\n\n\n\nЭкономические метрики\n\nтут будут расчеты экономических метрик\n\n\nЕсли расчет метрик является важным аспектом вашего исследования и требует подробного описания, то создание отдельной главы будет лучшим решением.\nЕсли в этом разделе будет немного расчетов, то можно сделать расчеты метрик разделом предобработки данных\n\n\nРасчитываем разные метрики на основе имеющихся данных и тех, которыми смогли обогатить данные\n\n\nВажно следить за количеством недель в году, если мы создаем столбец месяца.\nПроверять чтобы у нас не появлялась неделя дополнительная, из за того, что мы захватили предыдущий год\n\n\n\nКогортный анализ\n\nНе забывать про когортный анализ. Если у нас есть параметр, по которому мы можем наши данные разбить на когорты, то\nнужно разложить на когорты и посмотреть динамику по когортам.\nКогорты это например, пользователи пришедшие в одни день или месяц.\nЕсли мы объеденим пользователей в когорты и посмотрим динамику какого-то параметра по месяцам например, то увидим как изменяется.\nТут также нужно помнить, что если значение например за 3 месяц больше значения за 4 месяц, то это ничего не значит само по себе.\nТак как мы имеем дело с выборкой, то нам нужно проверить статистически значимая это разница.\nТут нам понядобятся стат тесты.\n\n\n\nПромежуточный вывод"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#анализ-корреляций-между-переменными",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#анализ-корреляций-между-переменными",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Анализ корреляций между переменными",
    "text": "Анализ корреляций между переменными\nВАЖНО\nпроверить, что все категориальные переменные по прежнему имеют категориальный тип, чтобы при анализе они не поетрялись\n\n\nShow the code\ndf.dtypes\n\n\n\nИсследование корреляционных связей\n\nТоп n значений одного столбца по значениям в другом Сделать функцию, чтобы в столбцах, где бльше 20 уникльных значений посмотреть топ n значений по другой колонке.\nНапример, топ 10 покупателей по сумме покупок и прочее.\nИдея в том, что если в столбце до 20 уникальных значений, то мы проанализируем комбинации с другими стобцами на графиках.\nА вот если у нас столбец не числовой и в нем больше 20 уникальных значений, то на графике мы не сможем понять топ n.\n\n\nИзучаем топ n значений в категориальных столбцах датафрейма, где значений больше порогового, по значению в столбце value_column.\nТут можно делать разные топы, использовать разные функции.\nЗадача изучить то, что мы не сможем изучить на графиках из-за болшого количества занчений в категориальной переменной,\nпоэтому мы берем топ n значений.\n\n\n\nShow the code\ngen = pagri_data_tools.top_n_values_gen()\nnext(gen)\n\n\n\nЧтобы сравнить метрики между собой мы можем - использовать корреляционный анализ (Пирсена, Спирмена, Кенделла)\n\n\nheatmap_corr(df)\n\n\n\nShow the code\npagri_data_tools.pagri_data_tools.heatmap_corr(df)\n\n\n\nИспользование регрессии и случайного леса для определения влияния переменных\n\n\nКоэффициенты регрессии позволяют оценить влияние каждой переменной на целевую переменную, учитывая влияние других переменных,\nв то время как важные компоненты в случайном лесе позволяют оценить важность каждой переменной для предсказания целевой переменной.\n\n\nИспользуем регрессиию\n\n\nЧтобы построить регрессию и посмотреть стат значимость и коэффициенты удобно использовать модуль statsmodel\n\n\nVIF означает Variance Inflation Factor (Фактор инфляции дисперсии). Это статистическая метрика,\nиспользуемая для обнаружения мультиколлинеарности (сильной корреляции) между предикторами (фичами) в линейной регрессии.\n\n\nОбычно, VIF интерпретируется следующим образом:\n\nVIF &lt; 5: слабая мультиколлинеарность\n5 ≤ VIF &lt; 10: умеренная мультиколлинеарность\nVIF ≥ 10: сильная мультиколлинеарность\n\n\n\nСмотрим R2 (коэффициент детерминации) - использовать коэффициенты у регресси Мы строим регрессию и смотрим, у каких метрик больше коэффициенты. Таким образом мы поймем какие метрики сильнее зависят с целевой.\nВажно, чтобы независимые переменные некоррелировали по отдельности и вместе (мультиколлиниарность).\nПо отдельности смотрим матрицу корреляции.\nЧтобы определить коррелириуют ли вместе, береме независимые переменные,\nи перебираем их выбирая одну из них целевой и смотрим R2.\nЕсли R2 большой, то значит эта метрика (которая целевая на этом шаге) хорошо описывается другими и ее можно выбросить. Также не забываем поправки на гетероскедостичность (HC0, HC1, HC2, HC3) в статпакетах.\nНам нужно ответить на следующие вопросы - Влияет ли метрика на целевую? Оцениваем коэффициенты в уравнении регресси у каждой метрики.\n- Как влияет метрика на целевую? Смотрим R2 (коэффициент детерминации). И определяем какая часть целевой переменной определяется независимыми метриками.\n- Коэффициенты при метриках в уравнении статистически значим? При какаом уровне значимости? Смотрим в стат пакете p value для каждого коэффициента, что нам говорит значим ли этот коэффициент.\nТо есть мы не просто смотрим его абсолютное значение, а учитываем p value.\n- Дайте содержательную интерпретацию коэффицентам? При увеличении метрики k на 1, целевая метрика увеличивается на \\(b_{k} * 1\\) То есть нужно перевести коэффициенты в реальное сравнение, насколько увелчисться целевая метрика при изменении определенной метрики на 1 - Найдите 95 процентный доверительный интервал. В стат пакете смотрим значение и оно говорит, что если мы многократно повторим ноши вычисления с новыми данными, то 95 процентов наших\nполученных коэффицентов будут лежать в этом диапазоне.\n\n\nСтроим модель и изучаем результат\nlinear_regression_with_vif\n\n\n\nShow the code\npagri_data_tools.linear_regression_with_vif()\n\n\n\nИспльзовать коэффициенты у классификацию\nСтроим случайный лес какие метрики сильнее всего влияют на решения модели.\nplot_feature_importances_classifier\nplot_feature_importances_regression\n\n\nТут нужно подумать как использовать категориальные переменные тоже\nНужно их перевести в one hot encoding или подобное, чтобы также проверить силу их влияния на целевую перменную\n\n\n\nShow the code\ntitles_for_axis = dict(\n    debt = 'долга'\n    , children = 'Кол-во детей'\n    , age = 'Возраст'\n    , total_income = 'Доход'\n)\n\npagri_data_tools.plot_feature_importances_classifier(df, target='debt', titles_for_axis=titles_for_axis)\npagri_data_tools.plot_feature_importances_regression()\n\n\n\nНа основе полученных данных формулируем гипотезы, которые будем проверять в блоке проверки гипотез\n\n\nиспользуем быблиотеку shap, чтобы определить метрики, которые лучше других помогают предсказывать целевую перемменную\n\n\n\nПромежуточный вывод"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#анализ-взаимосвязей-переменных-на-графиках",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#анализ-взаимосвязей-переменных-на-графиках",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Анализ взаимосвязей переменных на графиках",
    "text": "Анализ взаимосвязей переменных на графиках\nЕсли у нас много значений в переменной, то мы агрегируем данные и можем построить бары.\nНо если мы агрегируем данные по переменной, в которой много значений и нам это нужно.\nТо мы не сможем построить бары, и тогда мы строим гистограмму. То есть мы берем, например, для каждой заправки считаем среднее время заправки и так как у нас много заправок,\nно мы хотим визуализировать среднее время по ним, и не агрегировать по другому параметру, то мы можем испльзовать гистограмму.\nВ данном случае гистограмма своего рода агрегация в бины, то есть мы получаем как бы новую переменную из бинов, в каждом бине будет агрегированы данные.\nЭто работает, когда нам нужно просто посмотреть колечество, так как в бинах будет количество. Таким образом мы получаем сколько у нас заправок имеют определенное среднее вермя заправки.\n\nПро размер графиков\nСтандартный размер графиков width=600, height=400\nДля более сложных графиков, когда требуется больше места для отображения данных, можно использовать размеры width=800, height=600 или width=1000, height=800\n\n\nСравнивать количество элементов нужно в абсолютных и относительных величинах.\nКогда мы сравниваем только в абсолютных величинах, мы не учитываем размеры групп.\nВ одной группе может быть элементов больше чем в другой и тогда сравнение будет не совсем точным.\nЕсли у нас 2 категориальные переменные, то мы можем сравнивать отностельные величины\nпо одной переменной, а можем по другой.\nЭто как сравнивать суммарный возраст в группах, это не дает полной картины и мы сравниваем средний возраст,\nчтобы размер группы не влиял.\n\n\nВАЖНО Анализ графиков и выводы для них должны полностью перекрывать постановку задачи и цель.\nЭто значит, что если цель проанализировать зависимость наличия долга, то мы в идеале должны проанализировать\nвлиянеие каждой переменной на наличие долга (числовой и категориальной)\nКончено нужно проанализировать все возможные зависимости.\nНо все зависимости с переменной в постновке задачи мы обязаны проверить и дать выводы. И о наличии и об отсутствие.\nВажные выводы делаем не только о наличие интересных моментов, но и об отсутствие.\n\n\nСначала раздел графиков\nНа основе графиков формируются гипотезы (например, у нас у мужчин зп больше) И после раздела графиков идет раздел проверки гипотез. Тут мы првоеряем разные гипотезы новые и те, что увидели на графиках.\nЭто правильная последовательность сначала изучили графики и потом на основе их сформировали гипоетзы Перед разделом про графики идет раздел с корреляцией и поиском главных компонет случайного леса.\nМы выбиарем переменную, для которой мы далее хотим посмотреть разыне зависимости и указываем ее целевой для сучайного леса\nИ смотрим какие фичи сильнее влияют.\nИ теперь можем построить графики с целевой перменно и этими главными фичами и в выводе можно указать про то что это важные компоненты случаного леса\n\n\nНа основе полученных данных формулируем гипотезы, которые будем проверять в блоке проверки гипотез\n\n\nАнализ временных зависимостей\nКогда мы хотим изучить верменную зависимость, то нам нуно создать новые переменные с обрезанными (trunc or round) значениям, чтобы можно было сгруппировать используя groupby or pivot_table\nпо этой обрезанной переменной и применить функцию агрегации и построить график, например, среднее время заправки на азс по часам.\nВот когда мы работаем с временем, нам нужно думать какие переменные создать, обрезая текущее время.\n\nСтроим когортный анализ, если есть возможность\n\n\nЕсли у нас есть даты, то мы можем посмотреть не просто абсолютные значения на каждую дату какой-то метрики,\nа посмотреть относительные значения относительно предыдущего значения.\nДля этого нужно составить таблицу, в которой будет изменение в процентах относительно предыдущего значения.\nИ затем визуализировать для каждой даты динамику этого показателя\n\n\n\nИзучение зависимостей между числовыми переменными\n\n\nShow the code\nПостроить hexbinplot\n\n\n\nИзучаем scatter plots\n\n\n\nShow the code\ntitles_for_axis = dict(\n    # numeric column\n    children = 'Кол-во детей'\n    , age = 'Возраст'\n    , total_income = 'Доход'\n)\n\n\n\n\nShow the code\npagri_data_tools.pairplot(df, titles_for_axis=titles_for_axis, horizontal_spacing=0.12, height=400, width=1200, rows=1, cols=3).show(config=dict(displayModeBar=False, dpi=200), renderer=\"png\")\n# если нужно интерактивый график, то\nfig = pagri_data_tools.pairplot(df, titles_for_axis=titles_for_axis, horizontal_spacing=0.12, height=800, width=800)\nfig.show()\n\n\n\n\nИзучение зависимостей между категориальными переменными\n\n\nShow the code\ngen = pagri_graphs.categorical_graph_analys_gen(df)\n\n\n\n\nShow the code\nnext(gen)\n\n\n\nСтроим матрицу тепловой карты для категориальных переменных и изучаем зависимости\ncategorical_heatmap_matrix_gen\n\n\nПосмотрим на распределение количества элементов между группами\n\n\nНужно подумать как отобразить не только процент от всего количества, но и пороцент в группе\nТо есть у нас есть значение в ячейке, сумма всех, сумма по категории на оси x и сумма по категории на оси Y\nВот нужно как-то отобразить процент от суммы, процент от одной категории и от другой категории\n\n\n\nShow the code\n12 (0.5% of total, 20% of row, 15% of col)\n\n\n\nМожно сделать кнопки, чтобы можно было подсветку делать внури колонок и строк\n\n\nМожно сделать кнопки (процент от общего) (процент от тут указывается название оси x) (аналогично для второй оси)\n\n\n\nShow the code\ntitles_for_axis = dict(\n    # numeric column\n    children = ['Количество детей', 'количество детей', 0]\n    , age = ['Возраст, лет', 'возраст', 1]\n    , total_income = ['Ежемесячный доход', 'ежемесячный доход', 1]\n     # category column\n    , education = ['Уровень образования', 'уровня образования']\n    , family_status = ['Семейное положение', 'семейного положения']\n    , gender = ['Пол', 'пола']\n    , income_type = ['Тип занятости', 'типа занятости']\n    , debt = ['Задолженность', 'задолженности']\n    , purpose = ['Цель получения кредита', 'цели получения кредита']\n    , has_child = ['Наличие детей', 'наличия детей']\n    , age_cat = ['Возрастная категория, лет', 'возрастной категории']\n    , total_income_cat = ['Категория дохода', 'категории дохода']\n)\n\n\n\n\nShow the code\npagri_data_tools.categorical_graph_analys_gen()\n\n\n\nСтроим treemap\ntreemap\ntreemap_dash\napp = treemap_dash(df)\nif __name__ == '__main__':\n   app.run_server(debug=True)\n\n\n\nShow the code\npagri_data_tools.treemap()\n\n\n\n\nShow the code\napp = pagri_data_tools.treemap_dash(df)\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\n\nСтроим parallel_categories\nparallel_categories\nparallel_categories_dash\napp = treemap_dash(df)\nif __name__ == '__main__':\n   app.run_server(debug=True)\n\n\n\nShow the code\npagri_data_tools.parallel_categories()\n\n\n\n\nShow the code\napp = pagri_data_tools.parallel_categories_dash(df)\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\n\nСтроим Sankey\nsankey\nsankey_dash\napp = treemap_dash(df)\nif __name__ == '__main__':\n   app.run_server(debug=True)\n\n\n\nShow the code\npagri_data_tools.sankey()\n\n\n\n\nShow the code\napp = pagri_data_tools.sankey_dash(df)\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\n\n\nИзучение зависимостей между числовыми и категориальными переменными\n\nМожно добавить кнопку среднее и количество\nЧтобы можно было посмотртеть распределение по количеству, когда смотрить среднее.\n\n\ngraph_analysis\n\n\n\nShow the code\ngen = pagri_data_tools.graph_analysis()\n\n\n\nКогда выбрали нужные графики, то стоим их\n\n\nЧтобы автоматически генерировались подписи осей и заголовок графика, нужно заполшнить такой словарь.\nПервый элемент списка - это подпись оси\nВторой элемент списка - это как это название будет отображаться в заголовке графика\nДля числовых столбцов также указывается род, чтобы правильно выбрать (Середнее, средний, средняя) (0 - средний род, 1 - мужской род, 2 - женский род)\n\n\n\nShow the code\nf'Среднее / Медианное / Суммарное {numeric} в зависимости от {category} и {category}'\n\n\n\n\nShow the code\ntitles_for_axis = dict(\n    # numeric column (0 - средний род, 1 - мужской род, 2 - женский род) (Середнее образовние, средний доход, средняя температура) )\n    children = ['Количество детей', 'количество детей', 0]\n    , age = ['Возраст, лет', 'возраст', 1]\n    , total_income = ['Ежемесячный доход', 'ежемесячный доход', 1]\n     # category column\n    , education = ['Уровень образования', 'уровня образования']\n    , family_status = ['Семейное положение', 'семейного положения']\n    , gender = ['Пол', 'пола']\n    , income_type = ['Тип занятости', 'типа занятости']\n    , debt = ['Задолженность (1 - имеется, 0 - нет)', 'задолженности']\n    , purpose = ['Цель получения кредита', 'цели получения кредита']\n    , dob_cat = ['Возрастная категория, лет', 'возрастной категории']\n    , total_income_cat = ['Категория дохода', 'категории дохода']\n)\n\n\n\n\nShow the code\nsummory = []\ncolumns = []\n\n\n\n\nShow the code\ngen = pagri_data_tools.graph_analysis_gen(df)\n\n\n\nСначала запускаем код через 1 ячейку columns = next(gen),\nчтобы появлися график и в colunns появились текущие названия колонок\nДалее пишем наблюдения, если хотим сохранить график и выполняем ячейку ниже.\nЕсли сохранять не хотим, то просто выполняем дальше columns = next(gen)\n\n\n\nShow the code\nsummory.append(dict(\n    columns = columns\n    , observations =\n'''\n**Наблюдения:**\n- 21\n- Размер больше\n- 1\n- Сильнее других\n-\n'''))\n\n\n\n\nShow the code\ncolumns = next(gen)\n\n\n\nВ summory находятся названия колонок и наблюдения для графиков, которые стоит построить\n\n\nВАЖНО\nПосле построеня всех графиков и коппирования комментариев из summory\nМы в предваретельном выводе после раздела графиков вставляем выводы из summory\nЧтобы их не собирать вручную у каждого графика\n\n\n\nShow the code\ndef gen_temp():\n    for item in summory:\n        colunns = item['columns']\n        observations = item['observations']\n        print(observations)\n        yield colunns\n\n\n\n\nShow the code\ngen = gen_temp()\n\n\n\n\nShow the code\ncolunns = next(gen)\n\n\n\n**Наблюдения:**  \n- 21\n- Размер больше\n- 1\n- Сильнее других\n- \n\n\n\n\nЧтобы построить график без category- просто закоментируй строку с category.\n\n\n\nShow the code\nconfig = dict(df = df\n    , x = colunns[1]\n    , y = colunns[0]\n    , category = colunns[2]\n    # , width = 800\n    # , orientation = 'h'\n)\npagri_data_tools.bar(config, titles_for_axis)\n\n\nНаблюдения: - У мужчин средний доход выше\nНаблюдения: - текст\n\n\nПромежуточный вывод"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#формулирование-и-провера-гипотез",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#формулирование-и-провера-гипотез",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Формулирование и провера гипотез",
    "text": "Формулирование и провера гипотез\n\nФормулирование гипотез\n\nНа основе проведенного анализа данных сформулирем следующие гипотезы:\n\n\nГипотеза 1: Нет зависимость между наличием детей и возвратом кредита в срок.\nГипотеза 2: У мужчин средний доход выше\nГипотеза 3: Цель получения кредита не зависит от среднего ежемесяченого доход\nГипотеза 4: Средний доход по семейному статусу одинаковый, но у вдовцов отличается\nГипотеза 5: У должников в среднем больше детей\nГипотеза 6: У должников средний возраст ниже\nГипотеза 7: Медианный доход у должников и не должников не отличается\n\n\nНе забываем что гипотезы можно проверять и между 2 категориальными переменными.\nПроверять есть ли между ними зависимости.\nТакже если мы на графиках определили, что есть между 2 категориальными перменными связь, то тут можем это проверить\n\n\n\nПроверка гипотез\n\nПроверим сформулированные гипотезы.\n\n\nhypothesis Гипотеза 1: Название гипотезы\n\n\nH0: The two categorical variables are independent.\nH1: The two categorical variables are not independent (i.e., there is a significant association between them).\n\n\nПримеры гипотез - Есть ли зависимость между наличием детей и возвратом кредита в срок?\n\n\nЭто будет часто возникать когда у нас категориальная целевая переменная и другие категориальные переменные.\nИ мы хотим проверить влияют ли категориальные переменные на целевую.\nНапример у нас есть поле наличие долга (есть или нет)\nВот тут мы можем провести тесты со всеми каетгориями на наличие зависимости с наличием долга.\nВ идеале мы на графиках должны найти гипотезы и тут их проверить.\nНо если у нас целевая переменная, то мы можем сравнить ее со всеми категориями\n\n\n\nShow the code\npagri_data_tools.chi2_pearson()\n\n\n\nФормируем словарь для подписей осей и названия гистограм\n\n\n\nShow the code\ntitles_for_axis = dict(\n    # numeric column\n    children = ['Количество детей', 'количества детей']\n    , age = ['Возраст', 'возраста']\n    , total_income = ['Ежемесячный доход', 'ежемесячного дохода']\n)\n\n\n\nНулевая гипотеза должна быть направлена на отсутствие эффекта, а альтернативная гипотеза должна быть направлена на наличие эффекта.\n\n\nФормулируем гипотезу через H0, H1\n\n\nH0: У мужчин средний доход не выше, чем у женщин\nH1: У мужчин средний доход выше, чем у женщин\n\n\nСмотрим распределение метрики\n\n\n\nShow the code\npagri_data_tools.histogram(df.total_income, titles_for_axis)\n\n\n\nДелаем вывод о распределении.\nВыбираем критерий для проверки гипотезы.\nОпределяем уровнь значимости.\n\n\nПроводим тест\nВАЖНО\ndf в ttest_ind_df и подобных можно испльзовать только для 2 стороннего случая\nДля одностороннего нужно придумать как определять какая группа первая по порядку пойдет в тест,\nв зависимости от постановки гипотезы, так как альтернатива опредляется исходя из порядка аргументов в функции теста\n\n\nЕсли используем ттест или анову, то сначала проводим тест на проверку дисперсии\n\n\nH0: У должников и не должников дисперсия не отличается\nH1: У должников и не должников дисперсия отличается\n\n\n\nShow the code\npagri_data_tools.levene_df\npagri_data_tools.levene\npagri_data_tools.bartlett_df\npagri_data_tools.bartlett\n\n\n\nВыбираем критерий\n\n\n\nShow the code\npagri_data_tools.chi2_pearson\npagri_data_tools.ttest_ind_df\npagri_data_tools.ttest_ind\npagri_data_tools.mannwhitneyu_df\npagri_data_tools.mannwhitneyu\npagri_data_tools.proportion_ztest_1sample\npagri_data_tools.proportions_ztest_2sample\npagri_data_tools.proportions_ztest_column_2sample\npagri_data_tools.proportions_chi2\npagri_data_tools.proportions_chi2_column\npagri_data_tools.anova_oneway_df\npagri_data_tools.anova_oneway\npagri_data_tools.tukey_hsd_df\npagri_data_tools.anova_oneway_welch_df\npagri_data_tools.kruskal_df\npagri_data_tools.kruskal\npagri_data_tools.bootstrap_diff_2sample # важно, сохраняем fig и в следующей ячейке делаем fig.shwo(), иначе на google colab работает некорректно\n\n\n\nПроводим тест\n\n\nЕсли отклоняем гипотезу, то строим доверитлеьный интервал\n\n\n\nShow the code\npagri_data_tools.confint_t_2samples\npagri_data_tools.confint_t_2samples_df\npagri_data_tools.confint_proportion_ztest_2sample\npagri_data_tools.confint_proportion_ztest_column_2sample\npagri_data_tools.confint_proportion_2sample_statsmodels\npagri_data_tools.confint_proportion_coluns_2sample_statsmodels\n\n\n\n\nShow the code\ntitles_for_axis = dict(\n    # numeric column\n    children = ['Количество детей', 'количества детей']\n    , age = ['Возраст', 'возраста']\n    , total_income = ['Ежемесячный доход', 'ежемесячного дохода']\n)\n\n\n\nПроверим эти гипотезы\n\nГипотеза 1: Нет зависимость между наличием детей и возвратом кредита в срок\n\nH0: Наличие детей не влияет на возврат кредита в срок.\nH1: Наличие детей влияет на возврат кредита в срок.\n\n\nТак как у нас обе переменных категориальные, то воспользуемся критерием хи-квадрат Пирсона.\nУровень значимости alpha выберем 0.05\n\n\n\nShow the code\npagri_data_tools.chi2_pearson(df.has_child, df.debt)\n\n\nХи-квадрат Пирсона\nalpha =  0.05\np-value =  1.724356890544321e-05\nОтклоняем нулевую гипотезу, поскольку p-value меньше уровня значимости\n\n\nРезультат:\n&gt;На уровне значимости 0.05 гипотеза, что наличие детей не влияет на возврат кредита в срок не подтвердилась.\n\nСделать опцию в бутстреп функции, чтобы строился только доверительный интервал\n\n\nТакже сделать функцию для доверилеьных интервалов для мана уитни через\nthe Hodges-Lehmann estimation, which provides a point estimate and a confidence interval for the difference in medians.\n\n\n\nShow the code\nimport pingouin as pg\n\n# Perform the Mann-Whitney U test and calculate the confidence interval\nmw_test = pg.mwu(x, y, tail='two-sided', confidence=0.95)\n\n# Print the results\nprint(mw_test)\n\n\n\n\nShow the code\nimport numpy as np\nfrom scipy import stats\n\n# Perform the Mann-Whitney U test\nu_stat, p_value = stats.mannwhitneyu(x, y, alternative='two-sided')\n\n# Calculate the Hodges-Lehmann estimation\nhl_est = np.median(np.array([x_i - y_j for x_i in x for y_j in y]))\n\n# Calculate the confidence interval\nci = stats.t.interval(0.95, len(x) + len(y) - 2, loc=hl_est, scale=stats.sem(np.array([x_i - y_j for x_i in x for y_j in y])))\n\n# Print the results\nprint('Hodges-Lehmann estimation:', hl_est)\nprint('Confidence interval:', ci)\n\n\n\nПодход следуюищй - мы до раздела проверка гипотез, когда изучаем данные (разделы пропусков, выбросов, дубликатов, зависиместей между перменными и графики),\nто мы делаем выводы и формируем наблюдеия.\nВот эти наблюдения и выводы нужно проверить в проверке гипотез.\nИ потом в основном выводе уже писать не просто, что у нас мужчин больше чем женьшин, а писать, что на уровен значисомти таком то у нас мужчина больше чем\nженщин с таким то доверительным интервалом.\nТаким образом выводы по вомзожности должны проходить через этап проверки гипотез, тогда эти выводы становятся более существенными.\n\n\nГипотезы появляются, когда мы задаем вопросы данным. Мы изучили данные, преобработали и теперь начинаем задавать вопросы.\n\nВыдвигаем гипотезу (заметили что-то необычное и хотим проверить), далее формулируем ее и далее проверяем.\n\nНе забываем формулировать гипотезы словами. Пишем что является гипотезой H0, а что гипотезой H1\n\nФормулируем все гипотезы, которые хотим проверить. Если будет 100 гипотез, то все 100 нужно сформулировать и потом проверить и сделать вывод.\n\nГипотезы могут быть и простыми вопросами без гипотез H0 и H1, такие гипотезы мы проверяем графиками или анализируя таблицу.\n\nВосновном, когда мы собиаремся применить стат аппарат для проверки гипотезы, то мы должны записать ее через H0 и H1.\n\n\nАлгоритм проверки статистических гипотез\n\nпостановка задачи\n\nСформулировать, что мы хотим узнать о выборках с точки зрения бизнес задачи (равны ли средние доходы в группах)\nперевод бизнес-вопроса на язык статистики: средний доход в группах - проверка равенства средних значений\n\nформулировка гипотез\n\nформулировка нулевой гипотезы - с т.зр. равенства стат прараметров оцениваемых выборок\n(Н0: Средние траты клиентов по группе А равны средним тратам клинентов по группе В)\nформулировка альтернативной гипотезы - с точки зрения неравенства параметров\n(Н1: Средние траты клиентов по группе А не равны средним тратам клинентов по группе В)\n\nвыбор критерия alpha (почему 0.05 или 0.01)\n\nцена ошибки первого рода (при большой цене ошибки - в мед исследованиях, потенциальном ущербе ) - значение может быть больше, например 0.1\nв ежедневных бизнес задачах, обычно - 0.05\n\nанализ распределения\n\nвизуальная оценка\nследим за выбросами\nпроверка гипотез о типе распредеделения (например критерий Шапиро-Уилка)\nесли распределение не нормальное и размер выборки достаточный (больше 30-50 элементов)\nможет быть использован t-test именно для проверки гипотезы о равенстве средних.\nСогласно ЦПТ (центральная предельная теорема) средние этих выборок будут распределены нормально. См. статью Зотова\n\nвыбор критерия\n\nпри оценке равенства средних T-test или Welch T-test (если есть сомнения, то лучше Уэлча)\n\nпри рвенстве дисперсий используем обычный т тест\nесли дисперсии в выборках разные, то используем т теста Уэлча\n\n\nполучение результата\n\nрасчет p-value\n\nинтерпретация p-value\n\nсравнение p-value и alpha\nесли альфа &gt; p-value - отвергаем нулевую гипотезу\nесли альфа &lt; p-value - не можем отвергнуть нулевую гипотезу\n\n\n\n\nКакая у нас задача - Исследовать взаимосвязь между 2 переменными - обе переменные наминативные - Хи-квадрат Пирсона (не чувствителен к гетероскедастичности) (нормальность не обязательна) - обе переменные количественные - Коэффициент корреляции Пирсона (параметрика) (чувствителен к выбросам) (только непрерывные переменные) - Коэффициент корреляции Спирмена (чувствителен к выбросам) / Кендалла (менее чувствителен к выбросам) (непараметрика) (непрерывные переменные и порядковые категориальные переменные)\n- одна переменная номинативная (принимает 2 занчения), вторая количественная - значения - Т-критерий Стьюдента (параметрика) (желательно нормальность) (чувствителен к выбросам) (чувствителен к гетероскедастичности) - если дисперсии равны (тест левена, барлета) и количество в группах равно (тест на равенство пропорций), то используем обычный т тест (эта формула более точно даст результат для этого случая) - если дисперсии не равны (тест левена, барлета) или количество в группах не равно (тест на равенство пропорций), то используем тест Уэлча (эта формула использует больше неопределенности и лучше подходит для этого случая) - U-критерий Манна-Уитни (непараметрика) (нормальность не обязательна) (не чувствителен к гетероскедастичности) Если тестируемая фича полностью сдвигает выборку на некий коэффициент theta или масштабирует выборку на некий параметр theta (theta &gt; 0),\nто критерий Манна-Уитни применим - доли - Z тест для долей (параметрика) (желательно нормальность) (чувствителен к выбросам) (чувствителен к гетероскедастичности) - Chi-square тест для долей (непараметрика) (нормальность не обязательна) (не чувствителен к гетероскедастичности) - Исследовать взаимосвязь между несколькими переменными - Дисперсионный анализ (параметрика) (дисперсии в группах должны быть примерно равны) (желательно нормальность) (чувствителен к выбросам) (чувствителен к гетероскедастичности) - Welch’s ANOVA (устройчив к разной дисперсии в группах) (требует более больших размеров групп для точных результатов) (желательно нормальность) (чувствителен к выбросам) (не чувствителен к гетероскедастичности) - Критерий Краскела-Уоллиса (непараметрика) (нормальность не обязательна) (не чувствителен к гетероскедастичности) - Тест Тьюки (если anova или Краскела-Уоллиса нашил различия) (дисперсии в группах должны быть примерно равны) (параметрика) (желательно нормальность) (чувствителен к выбросам) (чувствителен к гетероскедастичности) - Проверить на равенство дисперсий в группах перед anova - Levene’s test (не требует нормальность) (менее чувствительный) - Bartlett’s test (требует нормальность) (более чувствительный)\n\n\nОтличная статья про доверительные интервалы для разных статистик\nhttps://habr.com/ru/articles/807051/\n\n\nBootstrapping\n\n\nВ бутстрепе, если мы хотим сравнить две выборки, то нельзя смотреть\nгде находится исходная разница средних в бутстрапированной выборке\nТак как мы берем бутстреп из наших выборок и впролне реально.что наша разность\nбудет близка к с реднему бутстропированной выборки\nПоэтому p value нужно определять по месту нуля в бутстропированной выборке\n\n\nПосмотрим p value для 0 (если различий нет, то разница должна быть 0) Для этого посчитаем cdf для + и - среднего, чтобы получить 2 значения cdf а теперь возьмем минимум и умножим на 2, так как альт гипотеза у нас.что просто не равно 0, значит и справа и слева\n\n\n\nShow the code\nEstimating the power of a non-parametric test using bootstrapping involves simulating the testing process multiple times to estimate the probability of rejecting the null hypothesis. Here's a general outline of the steps:\n\n**Specify the null and alternative hypotheses **: Define the null and alternative hypotheses for your test. For example, the null hypothesis might be that the two groups have the same distribution, and the alternative hypothesis might be that the two groups have different distributions.\n\nGenerate simulated data: Generate simulated data that reflects the null hypothesis. For example, you could generate two groups of random data from the same distribution.\n\nPerform the Mann-Whitney U test: Perform the Mann-Whitney U test on the simulated data to obtain a p-value.\n\nRepeat steps 2-3 many times: Repeat steps 2-3 many times (e.g., 1000 times) to generate a distribution of p-values under the null hypothesis.\n\nEstimate the power: Estimate the power of the test by calculating the proportion of times the p-value is below a certain significance level (e.g., 0.05) when the alternative hypothesis is true. To do this, you'll need to generate simulated data that reflects the alternative hypothesis and repeat steps 2-4.\n\n\n\n\nПромежуточный вывод"
  },
  {
    "objectID": "projects/housing-ads-investigation/housing_ads_investigation.html#общий-вывод",
    "href": "projects/housing-ads-investigation/housing_ads_investigation.html#общий-вывод",
    "title": "Исследование объявлений о продаже квартир",
    "section": "Общий вывод",
    "text": "Общий вывод\nВыводы:\n\nДолги есть у людей с разным доходом.\nУ должников в среднем больше детей.\nУ должников среднее количество детей больше у женщин, а у не должников срднее количество детей больше у мужчин\nУ должников средний возраст немного ниже для всех категорий семейного положения.\nМедианный доход у должников и не должников практически не отличается\nДолжники имеют ниже средний возраст как мужчины так и женщины. Ситуация сохраняется во всех группах дохода.\nЦель получения кредита практически не зависит от среднего ежемесяченого дохода.\n92 % клиентов не имеют долга.\nЛюди от 30 до 50 лет имеют самый высокий средний доход.\nБольше всего кредит берут на цели, связанные с недвижимостью, кроме людей в гражданском браке\nЛюди в гражданском браке чаще берут кредит на свадьбу\nЖенщины чаще возвращают кредит.\nАнализ значимости признаков для модели случайного леса показал, что доход является самым значимым признаком для предсказания задолженности.\n58 % клиентов либо женаты, либо замужем. 19 % в гражданском браке. Можно сделать вывод что большинство в браке.\n\nБольшинство клиентов женщины (66 процентов).\nТолько 5 процентов клиентов моложе 25 лет. Основная часть клиентов старше 30 лет.\nЧем меньше количество детей, тем больше значений с высоким доходом.\nБолшая часть женатых имеет доход 100-200 тыс\nНа всех уровнях образоания, кроме ученой степени, доход у мужчин выше.\nУ мужчин, которые в браке или были в браке, количество детей больше, чем у женщин в той же категории.\n\nАномалии и особенности в данных: - В датафрейме есть строки дубликаты. 54 строки. Меньше 1 % от всего датафрейма.\n- В столбце с количеством детей есть отрицательные значения. 47 штук. Меньше 1 процента от всего датафрейма. Также есть клиенты с 20 детьми. - Колонока общий трудовой стаж содержит 74 % отрицаетльных значений. А также максимальное количество дней стажа больше 400 тысяч дней, это больше 1000 лет. - В колонке возраста 101 нулевое значени. - Колонка дохода имеет слишком много знаков после запятой. - В колонке с образованием присутствуют одни и те же знчения с разными регистрами. При этом в колонке с id образования все впрорядке.\nРезультаты предобработки данных: - Удалили колонки с id образования и семейного статуса, так как нам для графиков лучше подойдут названия, а не id. - Колонка со стажем имеет совершенно некорректные данные. Чтобы не внести искажение в анализ, удалим эту колонку.\nРезультаты проверки гипотез:\n- Гипотеза 1: У мужчин средний доход выше, чем у женщин\n&gt; Результат: На уровне значимости 0.05 гипотеза подтвердилась. - Гипотеза 2: Цель получения кредита практически не зависит от среднего ежемесячного дохода\n&gt; Результат: На уровне значимости 0.05 у нас нет оснований отвергнуть гипотезу. - Гипотеза 3: Средний доход по семейному статусу примерно одинаковый, но у вдовцов отличается\n&gt; Результат: На уровне значимости 0.05 гипотеза подтвердилась. - Гипотеза 4: У должников в среднем больше детей\n&gt; Результат: На уровне значимости 0.05 гипотеза подтвердилась.\n- Гипотеза 5: У должников средний возраст ниже\n&gt; Результат: На уровне значимости 0.05 гипотеза подтвердилась. 95% доверительный интервал для разницы средних возрастов для должников и не должников равен (-inf, -2.74).\n- Гипотеза 6: Медианный доход у должников и не должников не отличается\n&gt; Результат: На уровне значимости 0.05 нет оснований отвергнуть гипотезу. 95% доверительный интервал разницы между медианным доходом должников и не должников равен (-2648.05, 179.34).\n- Гипотеза 7: Наличие детей не влияет на возврат кредита в срок\n&gt; Результат: На уровне значимости 0.05 гипотеза не подтвердилась.\nРекомендации: - Добавить контроль данных, чтобы не дублировались значения с разными регистрами в колонке с образованием. - Добавить уникальный идентификатор клиента, чтобы избежать дублирования строк. - Добавить проверку на отрицательные значения и на слишком болшьшие значения в количестве детей при загрузке данных. - Выяснить откуда возникают отрицательные значения в трудовом стаже и добавить контроль ввода невалидных данных. - Выяснить причину нулевых значений в колонке возраста и добавить проверку на нулевые значения при загрузке данных. - Выяснить причину большого количества знаков после запятой в колонке дохода.\n\nЧто нужно сообщить в выводе - информацию о том, что удалось подтвердить гипотезы (тут пишем только те, которые удалось подтвердить) - всю информацию о датасете, которые важны. Дубликаты, которые несут практическую пользу и рекомендации по ним, пропуски также с рекомендациями\nи остальные моменты по данным и рекомендации. Тут важно указывать именно найденные аномалии, которые имеют практическую пользу, которые нужно исправить и прочее.\nПишем, что были найдены выбросы, они были связаны возможно с тем то и тем то. - и в конце обязательно call to action написать что необходимо сделать с этими результатами\n\n\nСоветы по оформлению общего выывод - не нужно вставлять таблицы и графики в вывод. В выводе пишем словами самое важное и практически полезное, что мы получили, причем в порядке убывания важности.\nИ когда мы пишем, что увидели то-то, то приводим гиперссылку на график или результат ячейки, где это получено.\nТак будет компактный вывод и при необходимости человек сможет быстро перейти и посмотреть график или таблицу\n\nУдалось подтвердить гипотезу о влиянии различных характеристик клиента на факт погашения кредита в срок. Каждый из рассмотренных параметров оказывает влияние на надёжность заёмщика. Рассмотренные факторы по-разному влияют на надёжность заёмщиков. Например, семейное положение оказалось более значимым фактором, чем уровень дохода.\n\nВ ходе анализа исходного набора данных было проведено (были устранены пропуски в двух колонках с числовыми значениями - ‘total_income’ и ‘days_employed’).\n\nПосле устранения явных и скрытых дупликатов и удаления оставшихся после обогащения пропусков объем датасета сократился на 0.05%\nБыли устранены выбросы в колонках ‘days_employed’ и ‘children’: в первом случае выбросы возникли в результате системной ошибки (данные были внесены в часах, а не в днях); во втором случае ошибка, вероятнее всего была допущена людьми, вносившими данные в систему\n…\n\nНеобходимо\n\n\nЗапросить в отделе по работе с клиентами информацию о возможности брать кредит без подтверждения дохода.\nСообщить коллегам, занимающимся выгрузкой о наличие дубликатов, если вопрос не разрешится, запросить индентификационный номер клиента к датасету.\nПрописать в задаче на поставку данных формат данных (пол только F и M, положительные значения). Приложить информацию о найденных аномалиях.\n\n\nСначала проверяем орфографические ошибки\n\n\nShow the code\npagri_data_tools.correct_notebook_text()\n\n\nЗатем создаем номера у глав и оглавление\n\nЧтобы добавить номера глав и ссылки для оглавления и сделать оглавлнеие\nоглавление добавиться в начало ноутбука\n\n\nСначала можно в режиме draft сделать пробный варант, проверить и потом уже запустить в режиме final\n\n\n\nShow the code\npagri_data_tools.make_headers_link_and_toc()\n\n\nДалее создаем ссыки на выводы и аномалии\n\nЧтобы было удобно искать где вставить якорь для ссылки, названия выводов и аномалий должно точно совпадать\nв итоговом списке аномалий и выводов и в тех местах (то есть в наблюдениях под ячейками), куда мы будем помещать ссылки\n\n\nЧтобы сделать ссылки на выводы и аномалии, нужно\nв тех местах, куда хотим переходить по ссылке вставить текст выводов или аномалий (берем прямо из основных выводов)\nвыводы должны начинаться с _conclusion_\nаномалии должны начинаться с _anomaly_\nПримеры:\n\n\n\nShow the code\n_anomalies_ В столбце с количеством детей есть отрицательные значения. 47 штук.\n\n\n\nМожно в одной ячейке и выводы и аномалии, с обеих ссылок будет переходить сюда, но назад будет возвращаться только в одно место,\nв то, которое было первым в ячейке\n\n\n\nShow the code\n_conclusion_ Только 5 процентов клиентов моложе 25 лет. Основная часть клиентов старше 30 лет.\n_anomalies_ В колонке возраста 101 нулевое значени.\n\n\n\nСодеражние выводов и аномалий появится в начале ноутбука\nтакже 2 режима draft и final\n\n\nПодумать как сделать удобнее создание выводов\nПока лучше сначала взять выводы из наблюдений и выбрать из них наиболее важные и интересные, не меняя их.\nДалее берем этот список и поиском находим ячейку с этим выводом и перед графиком помещаем\nconclusion и сам вывод\n\n\nЧтобы был нужный порядок в списке выводов и аномалий в начале отчета, нужно передвать словарь со списками выводов и аномалий.\nПеременная order принимает словарь, где ключи onclusions и anomalies, а значения это соответствующие списки\n\n\nПримеры списков\n\n\n\nShow the code\norder = dict(\n            conclusions =[ 'Женщины чаще возвращают кредит, чем мужчины.']\n            , anomalies = ['В датафрейме есть строки дубликаты. 54 строки. Меньше 1 % от всего датафрейма.  ']\n)\n\n\n\n\nShow the code\npagri_data_tools.add_conclusions_and_anomalies()\n\n\nЕсли сильно нужно, создаем ссыки на гипотезы\n\nВ главе гипотез для каждой гипоетзы, куда будем переходить из оглавления, в начале перед гипотезой ставим hypothesis и пробел\n\n\n\nShow the code\n_hypothesis_ **Гипотеза 1: Название гипотезы**\n\n\n\nВыполняем следующую функцию и в начале отчета появится список гипотез с ссылками\nДалее нужно добавить результат гипотез вручную\n\n\n\nShow the code\npagri_data_tools.add_hypotheses_links_and_toc()\n\n\nФинальное размещение ноутбука на git hub с ссылкой на google colab\n\nКомитим на гит хаб финальную версию ноутбука.\nСоздаем на гит хаб readme файл проекта, в котором в начале идет ссылка на google colab\nДалее ее открываем и переходим на google colab\nВыполняем все ячейки, смотрим все ли правильно отобразилось.\nДалее в меню File выбираем сохранить копию на гит хаб.\nНе меняем имя, тогда все содержимое ноутбука сохраниться на гит хаб."
  }
]